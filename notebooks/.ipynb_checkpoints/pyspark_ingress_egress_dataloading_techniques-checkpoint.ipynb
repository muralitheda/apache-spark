{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d55ae10-939c-4905-8f70-7f2a73350867",
   "metadata": {},
   "source": [
    "# **PySpark Ingestion + Egress + Dataloading Techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d7fa8ec8-e8f6-4c26-a269-20bad66f4c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SparkSession Object Memory Reference: <pyspark.sql.session.SparkSession object at 0xffff706ed970>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#MySql jdbc connector jar local path\n",
    "mysql_connector_jar_path = \"/home/hduser/install/mysql-connector-java.jar\"\n",
    "\n",
    "#Spark Session Creation\n",
    "spark =  SparkSession.builder\\\n",
    "    .appName(\"Spark-Ingress-Egress-Dataloading-Practice\")\\\n",
    "    .config(\"spark.jars\", mysql_connector_jar_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"[INFO] SparkSession Object Memory Reference: {spark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f8fba-f5eb-422b-b9da-9a3aec6e3737",
   "metadata": {},
   "source": [
    "## **1. Reading a CSV data and write into MySql(RDBMS) Database using JDBC Option**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6c7fe856-1806-44e6-95d4-30b398a79831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[INFO] df1.count() = 9999\n"
     ]
    }
   ],
   "source": [
    "###### Reading CSV data and write into DataFrame #######\n",
    "\n",
    "# Sample Customer Info Data\n",
    "\"\"\"\n",
    "cd /home/hduser/custinfo.csv\n",
    "\n",
    "4000001,Kristina,Chung,55,Pilot\n",
    "4000002,Paige,Chen,77,Teacher\n",
    "4000003,Sherri,Melton,34,Firefighter\n",
    "4000004,Gretchen,Hill,66,Computer hardware engineer\n",
    "4000005,Karen,Puckett,74,Lawyer\n",
    "\"\"\"\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Schema Definition\n",
    "custinfo_schema = StructType([StructField('custid', IntegerType(), True), StructField('first_name', StringType(), True), StructField('last_name', StringType(), True), StructField('age', IntegerType(), True), StructField('profession', StringType(), True)])\n",
    "\n",
    "# CSV Data Read and storing it in DataFrame\n",
    "df1 = spark.read.csv(path=\"file:///home/hduser/custinfo.csv\",header=False,sep=\",\",inferSchema=False,schema=custinfo_schema)\n",
    "df1.show(truncate=False,n=5)\n",
    "print(f\"[INFO] df1.count() = {df1.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7d0aba4a-95f7-45ca-bfea-6324679cd6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CSV file data write into MySQL DB is successful.\n"
     ]
    }
   ],
   "source": [
    "###### Write the data into MySql DB ######\n",
    "\n",
    "# JDBC Options\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb?createDatabaseIfNotExist=true'\n",
    "dbproperties={'user':'root','password':'Root123$','driver':'com.mysql.cj.jdbc.Driver'}\n",
    "\n",
    "# Write into DB\n",
    "df1.write.jdbc(url=url1,properties=dbproperties,table=\"custinfo\",mode=\"overwrite\")\n",
    "print(\"[INFO] CSV file data write into MySQL DB is successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fac17921-76c7-44e6-8fe1-138577b7f6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Simple way to read the data from MySql/RDBMS DB using JDBC ######\n",
    "\n",
    "# JDBC Options\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb'\n",
    "dbproperties={'user':'root','password':'Root123$','driver':'com.mysql.cj.jdbc.Driver'}\n",
    "\n",
    "# Read the data from RDBMS using query instead of direct table\n",
    "table_query = \"(select * from stocksdb.custinfo) as tablename\"\n",
    "df2_db = spark.read.jdbc(url=url1,properties=dbproperties,table=table_query)\n",
    "df2_db.cache()\n",
    "df2_db.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "210d08d4-984d-4630-bbb3-6b70ac7637f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Optimized way to read the data from any RDBMS DB using JDBC ######\n",
    "\n",
    "#Question: How to improve performance for JDBC?\n",
    "#partition, fetchsize, caching, pushdown optimization etc.,\n",
    "#partitionColumn:, numberOfPartitions:, upperBound:, lowerBound, predicates, fetchsize..\n",
    "\n",
    "# JDBC Options for performance optimization\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb'\n",
    "dbproperties = {\n",
    "    'user': 'root',\n",
    "    'password': 'Root123$',\n",
    "    'driver': 'com.mysql.cj.jdbc.Driver',\n",
    "    # Performance optimization options (values as strings):\n",
    "    'partitionColumn': 'custid',\n",
    "    'lowerBound': '4000001',  # Column used to divide data into sections for parallel processing.\n",
    "    'upperBound': '4000100',  # Minimum value for the partition column to start reading data.\n",
    "    'numPartitions': '3',     # Maximum value for the partition column to start reading data.\n",
    "    'pushDownPredicate': 'true',  # Sends filters (WHERE clauses) to the database for early processing.\n",
    "    'pushDownAggregate': 'true',  # Sends aggregations (SUM, COUNT) to the database for early processing.\n",
    "    'queryTimeout': '120',    # Maximum time (in seconds) a database query can run before timing out.\n",
    "    'fetchSize': '10',        # Number of rows retrieved from the database in each batch.\n",
    "    'isolationLevel': 'READ_COMMITTED' # Ensures only committed data is visible during a transaction.\n",
    "}\n",
    "\n",
    "# Read the data from RDBMS using query instead of direct table\n",
    "table_query = \"(select * from stocksdb.custinfo) as tablename\"\n",
    "df2_db = spark.read.jdbc(url=url1,properties=dbproperties,table=table_query)\n",
    "df2_db.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c3ddf-7f63-496e-be76-c7515bee8d91",
   "metadata": {},
   "source": [
    "## **2. Schema Evoluation/Growing handling using columner file formats ORC/Parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8063682e-e89b-4789-9603-f9b093f76c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORC/PARQUET Other Properties\n",
    "\n",
    "#Source is sending data on a daily basis, once in a while the schema of the data is evolving/growing\n",
    "  #Example (Day1): exch~stock~price\n",
    "  #Example (Day2): exch~stock~price~buyer\n",
    "  #Example (Day3): stock~price~seller\n",
    "\n",
    "#**mergeSchema: Orc/Parquet read all the datafiles headers and merge them into one header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d797ccc0-0dda-499b-9e55-a70c698013df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Day1 : Source CSV data\n",
      "+----+-----+-----+\n",
      "|exch|stock|price|\n",
      "+----+-----+-----+\n",
      "|NYSE|  CLI| 36.3|\n",
      "|NYSE|  ABC| 36.3|\n",
      "+----+-----+-----+\n",
      "\n",
      "[INFO] Day1 : ORC data read\n",
      "+----+-----+-----+\n",
      "|exch|stock|price|\n",
      "+----+-----+-----+\n",
      "|NYSE|  CLI| 36.3|\n",
      "|NYSE|  ABC| 36.3|\n",
      "+----+-----+-----+\n",
      "\n",
      "[INFO] Day2 : Source CSV data\n",
      "+----+-----+-----+------+\n",
      "|exch|stock|price| buyer|\n",
      "+----+-----+-----+------+\n",
      "|NYSE|  CLI| 37.3|  Alan|\n",
      "|NYSE|  ABC| 37.3|Harpar|\n",
      "+----+-----+-----+------+\n",
      "\n",
      "[INFO] Day2 : ORC data read with evolved schema\n",
      "+----+-----+-----+------+\n",
      "|exch|stock|price| buyer|\n",
      "+----+-----+-----+------+\n",
      "|NYSE|  CLI| 37.3|  Alan|\n",
      "|NYSE|  ABC| 37.3|Harpar|\n",
      "|NYSE|  CLI| 36.3|  NULL|\n",
      "|NYSE|  ABC| 36.3|  NULL|\n",
      "+----+-----+-----+------+\n",
      "\n",
      "[INFO] Day3 : Source CSV data\n",
      "+-----+-----+------+\n",
      "|stock|price|seller|\n",
      "+-----+-----+------+\n",
      "|  CLI| 37.3|  Jack|\n",
      "|  ABC| 37.3|  Ross|\n",
      "+-----+-----+------+\n",
      "\n",
      "[INFO] Day3 : ORC data read evolved schema\n",
      "+----+-----+-----+------+------+\n",
      "|exch|stock|price| buyer|seller|\n",
      "+----+-----+-----+------+------+\n",
      "|NYSE|  CLI| 37.3|  Alan|  NULL|\n",
      "|NYSE|  ABC| 37.3|Harpar|  NULL|\n",
      "|NYSE|  CLI| 36.3|  NULL|  NULL|\n",
      "|NYSE|  ABC| 36.3|  NULL|  NULL|\n",
      "|NULL|  CLI| 37.3|  NULL|  Jack|\n",
      "|NULL|  ABC| 37.3|  NULL|  Ross|\n",
      "+----+-----+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "day1 = \"\"\"\n",
    "exch~stock~price\n",
    "NYSE~CLI~36.3\n",
    "NYSE~ABC~36.3\n",
    "\"\"\"\n",
    "\n",
    "day2 = \"\"\"\n",
    "exch~stock~price~buyer\n",
    "NYSE~CLI~37.3~Alan\n",
    "NYSE~ABC~37.3~Harpar\n",
    "\"\"\"\n",
    "\n",
    "day3 = \"\"\"\n",
    "stock~price~seller\n",
    "CLI~37.3~Jack\n",
    "ABC~37.3~Ross\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "/home/hduser/stockdata_csv/\n",
    "├── part-00000-01f262bb-27a7-465d-95ca-4fdb6e1986aa-c000.csv\n",
    "└── _SUCCESS\n",
    "\"\"\"\n",
    "\n",
    "# Write the same data into CSV + Read the CSV + Write into ORC format (Append) + Read the ORC data (MergeSchema=True) \n",
    "\n",
    "# Day 1: exch~stock~price\n",
    "lines_day1 = day1.strip().split('\\n')\n",
    "header_day1 = lines_day1[0].split('~')\n",
    "data_rows_day1 = [line.split('~') for line in lines_day1[1:]]\n",
    "df1 = spark.createDataFrame(data_rows_day1, header_day1)\n",
    "df1.coalesce(1).write.csv(path=\"file:///home/hduser/stockdata_csv/\",mode=\"overwrite\",sep=\"~\",header=True)\n",
    "\n",
    "df_csv = spark.read.csv(path=\"file:///home/hduser/stockdata_csv/\",pathGlobFilter=\"part-*.csv\",sep=\"~\",header=True)\n",
    "print(\"[INFO] Day1 : Source CSV data\")\n",
    "df_csv.show()\n",
    "df_csv.coalesce(1).write.orc(path=\"file:///home/hduser/stockdata_orc/\",mode=\"overwrite\") # Overwrite for the first time\n",
    "df_orc = spark.read.orc(path=\"file:///home/hduser/stockdata_orc/\",mergeSchema=True) # Schema Evoluation\n",
    "print(\"[INFO] Day1 : ORC data read\")\n",
    "df_orc.show()                         \n",
    "\n",
    "# Day 2: exch~stock~price~buyer\n",
    "lines_day2 = day2.strip().split('\\n')\n",
    "header_day2 = lines_day2[0].split('~')\n",
    "data_rows_day2 = [line.split('~') for line in lines_day2[1:]]\n",
    "df2 = spark.createDataFrame(data_rows_day2, header_day2)\n",
    "df2.coalesce(1).write.csv(path=\"file:///home/hduser/stockdata_csv/\",mode=\"overwrite\",sep=\"~\",header=True)\n",
    "\n",
    "df_csv = spark.read.csv(path=\"file:///home/hduser/stockdata_csv/\",pathGlobFilter=\"part-*.csv\",sep=\"~\",header=True)\n",
    "print(\"[INFO] Day2 : Source CSV data\")\n",
    "df_csv.show()\n",
    "df_csv.coalesce(1).write.orc(path=\"file:///home/hduser/stockdata_orc/\",mode=\"append\") # Append for the Schema Evoluation\n",
    "df_orc = spark.read.orc(path=\"file:///home/hduser/stockdata_orc/\",mergeSchema=True) # Schema Evoluation\n",
    "print(\"[INFO] Day2 : ORC data read with evolved schema\")\n",
    "df_orc.show()    \n",
    "\n",
    "# Day 3: stock~price~seller\n",
    "lines_day3 = day3.strip().split('\\n')\n",
    "header_day3 = lines_day3[0].split('~')\n",
    "data_rows_day3 = [line.split('~') for line in lines_day3[1:]]\n",
    "df3 = spark.createDataFrame(data_rows_day3, header_day3)\n",
    "print(\"[INFO] Day3 : Source CSV data\")\n",
    "df3.show()\n",
    "df3.coalesce(1).write.csv(path=\"file:///home/hduser/stockdata_csv/\",mode=\"overwrite\",sep=\"~\",header=True)\n",
    "\n",
    "df_csv = spark.read.csv(path=\"file:///home/hduser/stockdata_csv/\",pathGlobFilter=\"part-*.csv\",sep=\"~\",header=True)\n",
    "df_csv.coalesce(1).write.orc(path=\"file:///home/hduser/stockdata_orc/\",mode=\"append\") # Append for the Schema Evoluation\n",
    "df_orc = spark.read.orc(path=\"file:///home/hduser/stockdata_orc/\",mergeSchema=True) # Schema Evoluation\n",
    "print(\"[INFO] Day3 : ORC data read evolved schema\")\n",
    "df_orc.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704e59a-cbf4-4d70-bea9-584a877b04ec",
   "metadata": {},
   "source": [
    "## **3. Reading a JSON data with various options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "efbfbcc3-d829-430c-b989-5292b793a4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: decimal(10,3) (nullable = true)\n",
      " |-- isActive: boolean (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |-- date_joined: date (nullable = true)\n",
      " |-- timestamp_event: timestamp (nullable = true)\n",
      " |-- corrupted_record: string (nullable = true)\n",
      "\n",
      "+----+-------+----+---------+--------+-------------------------+------+-------------------------+-----------+-----------------------+----------------+\n",
      "|id  |name   |age |salary   |isActive|comments                 |tags  |address                  |date_joined|timestamp_event        |corrupted_record|\n",
      "+----+-------+----+---------+--------+-------------------------+------+-------------------------+-----------+-----------------------+----------------+\n",
      "|1   |Alice  |30  |50000.500|true    |This is a comment.       |[A, B]|{123 Main St, Anytown}   |NULL       |NULL                   |NULL            |\n",
      "|2   |Bob    |25  |45000.750|false   |Another comment.         |[C]   |{456 Oak Ave, Otherville}|NULL       |NULL                   |NULL            |\n",
      "|3   |Charlie|NULL|NULL     |true    |Invalid JSON             |[D, E]|NULL                     |NULL       |NULL                   |NULL            |\n",
      "|4   |David  |40  |60000.000|true    |NULL                     |NULL  |NULL                     |2023-01-15 |2023-01-15 10:30:00.123|NULL            |\n",
      "|5   |Eve    |35  |55555.555|true    |This has 'single quotes'.|NULL  |NULL                     |NULL       |NULL                   |NULL            |\n",
      "|6   |Frank  |28  |12345.678|true    |Escaped chars: \\n\\t\\r    |[F]   |NULL                     |NULL       |NULL                   |NULL            |\n",
      "|NULL|NULL   |NULL|NULL     |NULL    |NULL                     |NULL  |NULL                     |NULL       |NULL                   |NULL            |\n",
      "+----+-------+----+---------+--------+-------------------------+------+-------------------------+-----------+-----------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DecimalType,BooleanType,ArrayType,DateType,TimestampType\n",
    "\n",
    "# Data\n",
    "samplejson = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"name\": \"Alice\",\n",
    "    \"age\": 30,\n",
    "    \"salary\": 50000.50,\n",
    "    \"isActive\": true,\n",
    "    \"comments\": \"This is a comment.\",\n",
    "    \"tags\": [\"A\", \"B\"],\n",
    "    \"address\": {\n",
    "      \"street\": \"123 Main St\",\n",
    "      \"city\": \"Anytown\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2,\n",
    "    \"name\": \"Bob\",\n",
    "    \"age\": 25,\n",
    "    \"salary\": 45000.75,\n",
    "    \"isActive\": false,\n",
    "    \"comments\": \"Another comment.\",\n",
    "    \"tags\": [\"C\"],\n",
    "    \"address\": {\n",
    "      \"street\": \"456 Oak Ave\",\n",
    "      \"city\": \"Otherville\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": 3,\n",
    "    \"name\": \"Charlie\",\n",
    "    \"age\": null,\n",
    "    \"salary\": null,\n",
    "    \"isActive\": true,\n",
    "    \"comments\": \"Invalid JSON\",\n",
    "    \"tags\": [\"D\", \"E\"]\n",
    "  },\n",
    "  {\n",
    "    \"id\": 4,\n",
    "    \"name\": \"David\",\n",
    "    \"age\": 40,\n",
    "    \"salary\": 60000.00,\n",
    "    \"isActive\": true,\n",
    "    \"date_joined\": \"2023-01-15\",\n",
    "    \"timestamp_event\": \"2023-01-15 10:30:00.123\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 5,\n",
    "    \"name\": \"Eve\",\n",
    "    \"age\": 35,\n",
    "    \"salary\": 55555.555,\n",
    "    \"isActive\": true,\n",
    "    \"comments\": \"This has 'single quotes'.\",\n",
    "    \"field with space\": \"value\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 6,\n",
    "    \"name\": \"Frank\",\n",
    "    \"age\": 28,\n",
    "    \"salary\": 12345.678,\n",
    "    \"isActive\": true,\n",
    "    \"comments\": \"Escaped chars: \\\\n\\\\t\\\\r\",\n",
    "    \"tags\": [\"F\"],\n",
    "    \"decimal_val\": 12345.678\n",
    "  },\n",
    "  {\n",
    "    \"invalid\":\"invalid\"\n",
    "  }  \n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# JSON file creation\n",
    "import os\n",
    "file_path = \"/home/hduser/employe_json/sample.json\"\n",
    "directory = os.path.dirname(file_path)\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "with open(\"/home/hduser/employe_json/sample.json\", \"w\") as f:\n",
    "    f.write(samplejson)\n",
    "\n",
    "# Define a custom schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", DecimalType(10, 3), True),\n",
    "    StructField(\"isActive\", BooleanType(), True),\n",
    "    StructField(\"comments\", StringType(), True),\n",
    "    StructField(\"tags\", ArrayType(StringType()), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"street\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True)\n",
    "    ])),\n",
    "    StructField(\"date_joined\", DateType(), True),\n",
    "    StructField(\"timestamp_event\", TimestampType(), True),\n",
    "    StructField(\"corrupted_record\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Read JSON with multiple options and inline comments\n",
    "df = spark.read.json(\n",
    "    path=\"file:///home/hduser/employe_json/\",  # Specifies the location of the JSON file(s) to read.\n",
    "    schema=custom_schema,  # Defines a custom schema to avoid automatic inference.\n",
    "    primitivesAsString=False,  # Treats all primitive values (int, float, bool) as strings if True.\n",
    "    prefersDecimal=True,  # Infers floating-point numbers as DecimalType instead of DoubleType.\n",
    "    allowComments=True,  # Allows Java/C++ style comments (//, /* */) in JSON.\n",
    "    allowUnquotedFieldNames=True,  # Accepts JSON keys without double quotes.\n",
    "    allowSingleQuotes=True,  # Accepts single quotes for string values.\n",
    "    allowBackslashEscapingAnyCharacter=True,  # Allows any character to be escaped with a backslash.\n",
    "    mode=\"PERMISSIVE\",  # Defines how to handle corrupt records (PERMISSIVE, DROPMALFORMED, FAILFAST).\n",
    "    columnNameOfCorruptRecord=\"corrupted_record\",  # Stores malformed JSON strings in a specified column.\n",
    "    dateFormat=\"yyyy-MM-dd\",  # Specifies the format for parsing date strings.\n",
    "    timestampFormat=\"yyyy-MM-dd HH:mm:ss.SSS\",  # Specifies the format for parsing timestamp strings.\n",
    "    multiLine=True,  # Treats the entire file as a single JSON object (for pretty-printed or array JSON).\n",
    "    allowUnquotedControlChars=True,  # Allows control characters (e.g., \\n, \\t) to appear unquoted.\n",
    "    lineSep=\"\\n\",  # Defines a custom line separator between JSON records. If multiline=True then it is not required.\n",
    "    samplingRatio=1.0,  # Sets the fraction of data used for schema inference.\n",
    "    encoding=\"UTF-8\",  # Specifies the character encoding (e.g., UTF-8, UTF-16).\n",
    "    locale=\"en-US\",  # Sets the locale for parsing locale-sensitive data like dates.\n",
    "    pathGlobFilter=\"*.json\",  # Filters files using glob patterns (e.g., *.json).\n",
    "    recursiveFileLookup=True  # Enables recursive search in subdirectories.\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645d216-3f64-40c3-838e-d9a9c79372db",
   "metadata": {},
   "source": [
    "## **4. Reading a CSV data with various options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "315837f5-9161-4ae0-866a-453314f0feaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "|symbol                           |exchange|date      |timestamp          |price   |volume |corrupted_data                                     |\n",
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "|AAPL                             |NYSE    |2023-08-01|2023-08-01 09:30:00|195.25  |1200000|NULL                                               |\n",
      "|GOOGL                            |NYSE    |2023-08-01|2023-08-01 09:30:00|2735.55 |850000 |NULL                                               |\n",
      "|MSFT                             |NYSE    |2023-08-01|2023-08-01 09:30:00|NaN     |950000 |NULL                                               |\n",
      "|TSLA                             |NYSE    |2023-08-01|2023-08-01 09:30:00|Infinity|1100000|NULL                                               |\n",
      "|AMZN                             |NYSE    |NULL      |2023-08-01 09:30:00|134.25  |NULL   |NULL                                               |\n",
      "|MSFK                             |NYSE    |2023-08-01|NULL               |100.01  |950000 |MSFK,NYSE,2023-08-01,2023-08-01 09:30,100.01,950000|\n",
      "|INVALID_ROW_WITHOUT_PROPER_FIELDS|NULL    |NULL      |NULL               |NULL    |NULL   |INVALID_ROW_WITHOUT_PROPER_FIELDS                  |\n",
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "\n",
      "[INFO] Corruputed Rows\n",
      "+---------------------------------+--------+----------+---------+------+------+---------------------------------------------------+\n",
      "|symbol                           |exchange|date      |timestamp|price |volume|corrupted_data                                     |\n",
      "+---------------------------------+--------+----------+---------+------+------+---------------------------------------------------+\n",
      "|MSFK                             |NYSE    |2023-08-01|NULL     |100.01|950000|MSFK,NYSE,2023-08-01,2023-08-01 09:30,100.01,950000|\n",
      "|INVALID_ROW_WITHOUT_PROPER_FIELDS|NULL    |NULL      |NULL     |NULL  |NULL  |INVALID_ROW_WITHOUT_PROPER_FIELDS                  |\n",
      "+---------------------------------+--------+----------+---------+------+------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Data\n",
    "data = \"\"\"\n",
    "symbol,exchange,date,timestamp,price,volume\n",
    "AAPL,NYSE,2023-08-01,2023-08-01 09:30:00,195.25,1200000\n",
    "GOOGL,NYSE,2023-08-01,2023-08-01 09:30:00,2735.55,850000\n",
    "MSFT,NYSE,2023-08-01,2023-08-01 09:30:00,-1,950000\n",
    "TSLA,NYSE,2023-08-01,2023-08-01 09:30:00,Inf,1100000\n",
    "AMZN,NYSE,na,2023-08-01 09:30:00,134.25,na\n",
    "MSFK,NYSE,2023-08-01,2023-08-01 09:30,100.01,950000\n",
    "INVALID_ROW_WITHOUT_PROPER_FIELDS       \n",
    "\"\"\"\n",
    "\n",
    "# CSV file creation\n",
    "file_path = \"/home/hduser/employe_csv/sample.csv\"\n",
    "directory = os.path.dirname(file_path)\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(data)\n",
    "\n",
    "# Define custom schema\n",
    "customschema = StructType([\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"exchange\", StringType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"volume\", IntegerType(), True),\n",
    "    StructField(\"corrupted_data\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read CSV with various options\n",
    "df1 = spark.read.csv(\n",
    "    path=\"file:///home/hduser/employe_csv/sample.csv\",\n",
    "    sep=',',  # Column separator used in the CSV file\n",
    "    header=True,  # First line of the file contains column headers\n",
    "    schema=customschema,  # Custom schema to define data types and structure\n",
    "    columnNameOfCorruptRecord='corrupted_data',  # Stores malformed rows in this column\n",
    "    encoding='UTF-8',  # Character encoding used to read the file\n",
    "    quote=\"'\",  # Defines single quote as the string quoting character\n",
    "    comment='-',  # Lines starting with '-' are treated as comments and ignored\n",
    "    ignoreTrailingWhiteSpace=True,  # Trims trailing whitespace from fields\n",
    "    ignoreLeadingWhiteSpace=True,  # Trims leading whitespace from fields\n",
    "    nullValue='na',  # Treats 'na' as a null value\n",
    "    nanValue='-1',  # Treats '-1' as NaN (Not a Number)\n",
    "    positiveInf='Inf',  # Treats 'Inf' as positive infinity\n",
    "    dateFormat='yyyy-MM-dd',  # Format used to parse date fields\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',  # Format used to parse timestamp fields\n",
    "    maxColumns=40  # Maximum number of columns allowed in the file\n",
    ")\n",
    "\n",
    "# Show first 10 rows\n",
    "df1.show(10, False)\n",
    "\n",
    "print(\"[INFO] Corruputed Rows\")\n",
    "# Cache and filter corrupted rows\n",
    "df2 = df1.cache().where(\"corrupted_data is not null\")\n",
    "df2.show(10, False)  # Display malformed rows for RCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de623630-d063-4ca1-a4c0-437102f7e94c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
