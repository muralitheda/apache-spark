{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d55ae10-939c-4905-8f70-7f2a73350867",
   "metadata": {},
   "source": [
    "# **PySpark Ingestion + Egress + Dataloading Techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7fa8ec8-e8f6-4c26-a269-20bad66f4c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SparkSession Object Memory Reference: <pyspark.sql.session.SparkSession object at 0xffff4557f3d0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#MySql jdbc connector jar local path\n",
    "mysql_connector_jar_path = \"/home/hduser/install/mysql-connector-java.jar\"\n",
    "\n",
    "#Spark Session Creation\n",
    "spark =  SparkSession.builder\\\n",
    "    .appName(\"Spark-Ingress-Egress-Dataloading-Practice\")\\\n",
    "    .config(\"spark.jars\", mysql_connector_jar_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"[INFO] SparkSession Object Memory Reference: {spark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54905feb-da58-4961-a550-9f7dd7eb23d5",
   "metadata": {},
   "source": [
    "## **1. Converting Unstructured/Semi Structured Data to Structured Data using RDD then to DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5421c0a6-2025-44de-beb7-57e26fe2ab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "struct_rdd2 =>  ['Python', 'Cloud', 'AI', 'Docker', 'Kubernetes']\n",
      "struct_schema_rdd3 =>  [['Python'], ['Cloud'], ['AI'], ['Docker'], ['Kubernetes']]\n",
      "+----------+\n",
      "|        _1|\n",
      "+----------+\n",
      "|    Python|\n",
      "|     Cloud|\n",
      "|        AI|\n",
      "|    Docker|\n",
      "|Kubernetes|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Case1: Users entered course data in Unstructured format. We would like to know the distinct number of course, which course mostly wanted?\n",
    "\n",
    "#Sample data => samplecourse.log\n",
    "\"\"\"\n",
    "Docker Java JavaScript React Spark Kafka SQL Git Go Python AI\n",
    "\n",
    "AWS Azure TensorFlow PyTorch Android iOS Rust AI\n",
    "Python Cloud AI Docker Kubernetes Rust\n",
    "\"\"\"\n",
    "\n",
    "#1. Covert the unstructured to structured data using RDD\n",
    "sc = spark.sparkContext\n",
    "unstruct_rdd1 = sc.textFile(\"file:///home/hduser/samplecourse.log\")\n",
    "struct_rdd2 = unstruct_rdd1.flatMap(lambda row:row.split(\" \"))\n",
    "print(\"struct_rdd2 => \", struct_rdd2.take(5))\n",
    "struct_schema_rdd3 = struct_rdd2.map(lambda word:[word])\n",
    "print(\"struct_schema_rdd3 => \",struct_schema_rdd3.take(5))\n",
    "\n",
    "#2. Covert the RDD into DF for further analysis\n",
    "df1 = struct_schema_rdd3.toDF()\n",
    "df1.show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1051f-e941-477e-9fde-65cc2b12881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case2: System log data in Unstrucrured format. How that be converted to DF for futher analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f8fba-f5eb-422b-b9da-9a3aec6e3737",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **2. Reading a CSV data and write into MySql(RDBMS) Database using JDBC Option**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c7fe856-1806-44e6-95d4-30b398a79831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[INFO] df1.count() = 9999\n"
     ]
    }
   ],
   "source": [
    "###### Reading CSV data and write into DataFrame #######\n",
    "\n",
    "# Sample Customer Info Data\n",
    "\"\"\"\n",
    "cd /home/hduser/custinfo.csv\n",
    "\n",
    "4000001,Kristina,Chung,55,Pilot\n",
    "4000002,Paige,Chen,77,Teacher\n",
    "4000003,Sherri,Melton,34,Firefighter\n",
    "4000004,Gretchen,Hill,66,Computer hardware engineer\n",
    "4000005,Karen,Puckett,74,Lawyer\n",
    "\"\"\"\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Schema Definition\n",
    "custinfo_schema = StructType([StructField('custid', IntegerType(), True), StructField('first_name', StringType(), True), StructField('last_name', StringType(), True), StructField('age', IntegerType(), True), StructField('profession', StringType(), True)])\n",
    "\n",
    "# CSV Data Read and storing it in DataFrame\n",
    "df1 = spark.read.csv(path=\"file:///home/hduser/custinfo.csv\",header=False,sep=\",\",inferSchema=False,schema=custinfo_schema)\n",
    "df1.show(truncate=False,n=5)\n",
    "print(f\"[INFO] df1.count() = {df1.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0aba4a-95f7-45ca-bfea-6324679cd6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CSV file data write into MySQL DB is successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "###### Write the data into MySql DB ######\n",
    "\n",
    "# JDBC Options\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb?createDatabaseIfNotExist=true'\n",
    "dbproperties={'user':'root','password':'Root123$','driver':'com.mysql.cj.jdbc.Driver'}\n",
    "\n",
    "# Write into DB\n",
    "df1.write.jdbc(url=url1,properties=dbproperties,table=\"custinfo\",mode=\"overwrite\")\n",
    "print(\"[INFO] CSV file data write into MySQL DB is successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac17921-76c7-44e6-8fe1-138577b7f6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Simple way to read the data from MySql/RDBMS DB using JDBC ######\n",
    "\n",
    "# JDBC Options\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb'\n",
    "dbproperties={'user':'root','password':'Root123$','driver':'com.mysql.cj.jdbc.Driver'}\n",
    "\n",
    "# Read the data from RDBMS using query instead of direct table\n",
    "table_query = \"(select * from stocksdb.custinfo) as tablename\"\n",
    "df2_db = spark.read.jdbc(url=url1,properties=dbproperties,table=table_query)\n",
    "df2_db.cache()\n",
    "df2_db.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "210d08d4-984d-4630-bbb3-6b70ac7637f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Optimized way to read the data from any RDBMS DB using JDBC ######\n",
    "\n",
    "#Question: How to improve performance for JDBC?\n",
    "#partition, fetchsize, caching, pushdown optimization etc.,\n",
    "#partitionColumn:, numberOfPartitions:, upperBound:, lowerBound, predicates, fetchsize..\n",
    "\n",
    "# JDBC Options for performance optimization\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb'\n",
    "dbproperties = {\n",
    "    'user': 'root',\n",
    "    'password': 'Root123$',\n",
    "    'driver': 'com.mysql.cj.jdbc.Driver',\n",
    "    # Performance optimization options (values as strings):\n",
    "    'partitionColumn': 'custid',\n",
    "    'lowerBound': '4000001',  # Column used to divide data into sections for parallel processing.\n",
    "    'upperBound': '4000100',  # Minimum value for the partition column to start reading data.\n",
    "    'numPartitions': '3',     # Maximum value for the partition column to start reading data.\n",
    "    'pushDownPredicate': 'true',  # Sends filters (WHERE clauses) to the database for early processing.\n",
    "    'pushDownAggregate': 'true',  # Sends aggregations (SUM, COUNT) to the database for early processing.\n",
    "    'queryTimeout': '120',    # Maximum time (in seconds) a database query can run before timing out.\n",
    "    'fetchSize': '10',        # Number of rows retrieved from the database in each batch.\n",
    "    'isolationLevel': 'READ_COMMITTED' # Ensures only committed data is visible during a transaction.\n",
    "}\n",
    "\n",
    "# Read the data from RDBMS using query instead of direct table\n",
    "table_query = \"(select * from stocksdb.custinfo) as tablename\"\n",
    "df2_db = spark.read.jdbc(url=url1,properties=dbproperties,table=table_query)\n",
    "df2_db.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c3ddf-7f63-496e-be76-c7515bee8d91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **3. Schema Evoluation/Growing handling using columner file formats ORC/Parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8063682e-e89b-4789-9603-f9b093f76c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORC/PARQUET Other Properties\n",
    "\n",
    "#Source is sending data on a daily basis, once in a while the schema of the data is evolving/growing\n",
    "  #Example (Day1): exch~stock~price\n",
    "  #Example (Day2): exch~stock~price~buyer\n",
    "  #Example (Day3): stock~price~seller\n",
    "\n",
    "#**mergeSchema: Orc/Parquet read all the datafiles headers and merge them into one header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d797ccc0-0dda-499b-9e55-a70c698013df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Day1 : Source CSV data\n",
      "+----+-----+-----+\n",
      "|exch|stock|price|\n",
      "+----+-----+-----+\n",
      "|NYSE|  CLI| 36.3|\n",
      "|NYSE|  ABC| 36.3|\n",
      "+----+-----+-----+\n",
      "\n",
      "[INFO] Day1 : ORC data read\n",
      "+----+-----+-----+\n",
      "|exch|stock|price|\n",
      "+----+-----+-----+\n",
      "|NYSE|  CLI| 36.3|\n",
      "|NYSE|  ABC| 36.3|\n",
      "+----+-----+-----+\n",
      "\n",
      "[INFO] Day2 : Source CSV data\n",
      "+----+-----+-----+------+\n",
      "|exch|stock|price| buyer|\n",
      "+----+-----+-----+------+\n",
      "|NYSE|  CLI| 37.3|  Alan|\n",
      "|NYSE|  ABC| 37.3|Harpar|\n",
      "+----+-----+-----+------+\n",
      "\n",
      "[INFO] Day2 : ORC data read with evolved schema\n",
      "+----+-----+-----+------+\n",
      "|exch|stock|price| buyer|\n",
      "+----+-----+-----+------+\n",
      "|NYSE|  CLI| 37.3|  Alan|\n",
      "|NYSE|  ABC| 37.3|Harpar|\n",
      "|NYSE|  CLI| 36.3|  NULL|\n",
      "|NYSE|  ABC| 36.3|  NULL|\n",
      "+----+-----+-----+------+\n",
      "\n",
      "[INFO] Day3 : Source CSV data\n",
      "+-----+-----+------+\n",
      "|stock|price|seller|\n",
      "+-----+-----+------+\n",
      "|  CLI| 37.3|  Jack|\n",
      "|  ABC| 37.3|  Ross|\n",
      "+-----+-----+------+\n",
      "\n",
      "[INFO] Day3 : ORC data read evolved schema\n",
      "+----+-----+-----+------+------+\n",
      "|exch|stock|price| buyer|seller|\n",
      "+----+-----+-----+------+------+\n",
      "|NYSE|  CLI| 37.3|  Alan|  NULL|\n",
      "|NYSE|  ABC| 37.3|Harpar|  NULL|\n",
      "|NYSE|  CLI| 36.3|  NULL|  NULL|\n",
      "|NYSE|  ABC| 36.3|  NULL|  NULL|\n",
      "|NULL|  CLI| 37.3|  NULL|  Jack|\n",
      "|NULL|  ABC| 37.3|  NULL|  Ross|\n",
      "+----+-----+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "day1 = \"\"\"\n",
    "exch~stock~price\n",
    "NYSE~CLI~36.3\n",
    "NYSE~ABC~36.3\n",
    "\"\"\"\n",
    "\n",
    "day2 = \"\"\"\n",
    "exch~stock~price~buyer\n",
    "NYSE~CLI~37.3~Alan\n",
    "NYSE~ABC~37.3~Harpar\n",
    "\"\"\"\n",
    "\n",
    "day3 = \"\"\"\n",
    "stock~price~seller\n",
    "CLI~37.3~Jack\n",
    "ABC~37.3~Ross\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "/home/hduser/stockdata_csv/\n",
    "├── part-00000-01f262bb-27a7-465d-95ca-4fdb6e1986aa-c000.csv\n",
    "└── _SUCCESS\n",
    "\"\"\"\n",
    "\n",
    "# Write the same data into CSV + Read the CSV + Write into ORC format (Append) + Read the ORC data (MergeSchema=True) \n",
    "\n",
    "# Day 1: exch~stock~price\n",
    "lines_day1 = day1.strip().split('\\n')\n",
    "header_day1 = lines_day1[0].split('~')\n",
    "data_rows_day1 = [line.split('~') for line in lines_day1[1:]]\n",
    "df1 = spark.createDataFrame(data_rows_day1, header_day1)\n",
    "df1.coalesce(1).write.csv(path=\"file:///home/hduser/stockdata_csv/\",mode=\"overwrite\",sep=\"~\",header=True)\n",
    "\n",
    "df_csv = spark.read.csv(path=\"file:///home/hduser/stockdata_csv/\",pathGlobFilter=\"part-*.csv\",sep=\"~\",header=True)\n",
    "print(\"[INFO] Day1 : Source CSV data\")\n",
    "df_csv.show()\n",
    "df_csv.coalesce(1).write.orc(path=\"file:///home/hduser/stockdata_orc/\",mode=\"overwrite\") # Overwrite for the first time\n",
    "df_orc = spark.read.orc(path=\"file:///home/hduser/stockdata_orc/\",mergeSchema=True) # Schema Evoluation\n",
    "print(\"[INFO] Day1 : ORC data read\")\n",
    "df_orc.show()                         \n",
    "\n",
    "# Day 2: exch~stock~price~buyer\n",
    "lines_day2 = day2.strip().split('\\n')\n",
    "header_day2 = lines_day2[0].split('~')\n",
    "data_rows_day2 = [line.split('~') for line in lines_day2[1:]]\n",
    "df2 = spark.createDataFrame(data_rows_day2, header_day2)\n",
    "df2.coalesce(1).write.csv(path=\"file:///home/hduser/stockdata_csv/\",mode=\"overwrite\",sep=\"~\",header=True)\n",
    "\n",
    "df_csv = spark.read.csv(path=\"file:///home/hduser/stockdata_csv/\",pathGlobFilter=\"part-*.csv\",sep=\"~\",header=True)\n",
    "print(\"[INFO] Day2 : Source CSV data\")\n",
    "df_csv.show()\n",
    "df_csv.coalesce(1).write.orc(path=\"file:///home/hduser/stockdata_orc/\",mode=\"append\") # Append for the Schema Evoluation\n",
    "df_orc = spark.read.orc(path=\"file:///home/hduser/stockdata_orc/\",mergeSchema=True) # Schema Evoluation\n",
    "print(\"[INFO] Day2 : ORC data read with evolved schema\")\n",
    "df_orc.show()    \n",
    "\n",
    "# Day 3: stock~price~seller\n",
    "lines_day3 = day3.strip().split('\\n')\n",
    "header_day3 = lines_day3[0].split('~')\n",
    "data_rows_day3 = [line.split('~') for line in lines_day3[1:]]\n",
    "df3 = spark.createDataFrame(data_rows_day3, header_day3)\n",
    "print(\"[INFO] Day3 : Source CSV data\")\n",
    "df3.show()\n",
    "df3.coalesce(1).write.csv(path=\"file:///home/hduser/stockdata_csv/\",mode=\"overwrite\",sep=\"~\",header=True)\n",
    "\n",
    "df_csv = spark.read.csv(path=\"file:///home/hduser/stockdata_csv/\",pathGlobFilter=\"part-*.csv\",sep=\"~\",header=True)\n",
    "df_csv.coalesce(1).write.orc(path=\"file:///home/hduser/stockdata_orc/\",mode=\"append\") # Append for the Schema Evoluation\n",
    "df_orc = spark.read.orc(path=\"file:///home/hduser/stockdata_orc/\",mergeSchema=True) # Schema Evoluation\n",
    "print(\"[INFO] Day3 : ORC data read evolved schema\")\n",
    "df_orc.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704e59a-cbf4-4d70-bea9-584a877b04ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **4. Reading a JSON data with various options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efbfbcc3-d829-430c-b989-5292b793a4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: decimal(10,3) (nullable = true)\n",
      " |-- isActive: boolean (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |-- date_joined: date (nullable = true)\n",
      " |-- timestamp_event: timestamp (nullable = true)\n",
      " |-- corrupted_record: string (nullable = true)\n",
      "\n",
      "+----+-------+----+---------+--------+-------------------------+------+-------------------------+-----------+-----------------------+----------------+\n",
      "|id  |name   |age |salary   |isActive|comments                 |tags  |address                  |date_joined|timestamp_event        |corrupted_record|\n",
      "+----+-------+----+---------+--------+-------------------------+------+-------------------------+-----------+-----------------------+----------------+\n",
      "|1   |Alice  |30  |50000.500|true    |This is a comment.       |[A, B]|{123 Main St, Anytown}   |NULL       |NULL                   |NULL            |\n",
      "|2   |Bob    |25  |45000.750|false   |Another comment.         |[C]   |{456 Oak Ave, Otherville}|NULL       |NULL                   |NULL            |\n",
      "|3   |Charlie|NULL|NULL     |true    |Invalid JSON             |[D, E]|NULL                     |NULL       |NULL                   |NULL            |\n",
      "|4   |David  |40  |60000.000|true    |NULL                     |NULL  |NULL                     |2023-01-15 |2023-01-15 10:30:00.123|NULL            |\n",
      "|5   |Eve    |35  |55555.555|true    |This has 'single quotes'.|NULL  |NULL                     |NULL       |NULL                   |NULL            |\n",
      "|6   |Frank  |28  |12345.678|true    |Escaped chars: \\n\\t\\r    |[F]   |NULL                     |NULL       |NULL                   |NULL            |\n",
      "|NULL|NULL   |NULL|NULL     |NULL    |NULL                     |NULL  |NULL                     |NULL       |NULL                   |NULL            |\n",
      "+----+-------+----+---------+--------+-------------------------+------+-------------------------+-----------+-----------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DecimalType,BooleanType,ArrayType,DateType,TimestampType\n",
    "\n",
    "# Data\n",
    "samplejson = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"name\": \"Alice\",\n",
    "    \"age\": 30,\n",
    "    \"salary\": 50000.50,\n",
    "    \"isActive\": true,\n",
    "    \"comments\": \"This is a comment.\",\n",
    "    \"tags\": [\"A\", \"B\"],\n",
    "    \"address\": {\n",
    "      \"street\": \"123 Main St\",\n",
    "      \"city\": \"Anytown\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2,\n",
    "    \"name\": \"Bob\",\n",
    "    \"age\": 25,\n",
    "    \"salary\": 45000.75,\n",
    "    \"isActive\": false,\n",
    "    \"comments\": \"Another comment.\",\n",
    "    \"tags\": [\"C\"],\n",
    "    \"address\": {\n",
    "      \"street\": \"456 Oak Ave\",\n",
    "      \"city\": \"Otherville\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": 3,\n",
    "    \"name\": \"Charlie\",\n",
    "    \"age\": null,\n",
    "    \"salary\": null,\n",
    "    \"isActive\": true,\n",
    "    \"comments\": \"Invalid JSON\",\n",
    "    \"tags\": [\"D\", \"E\"]\n",
    "  },\n",
    "  {\n",
    "    \"id\": 4,\n",
    "    \"name\": \"David\",\n",
    "    \"age\": 40,\n",
    "    \"salary\": 60000.00,\n",
    "    \"isActive\": true,\n",
    "    \"date_joined\": \"2023-01-15\",\n",
    "    \"timestamp_event\": \"2023-01-15 10:30:00.123\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 5,\n",
    "    \"name\": \"Eve\",\n",
    "    \"age\": 35,\n",
    "    \"salary\": 55555.555,\n",
    "    \"isActive\": true,\n",
    "    \"comments\": \"This has 'single quotes'.\",\n",
    "    \"field with space\": \"value\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 6,\n",
    "    \"name\": \"Frank\",\n",
    "    \"age\": 28,\n",
    "    \"salary\": 12345.678,\n",
    "    \"isActive\": true,\n",
    "    \"comments\": \"Escaped chars: \\\\n\\\\t\\\\r\",\n",
    "    \"tags\": [\"F\"],\n",
    "    \"decimal_val\": 12345.678\n",
    "  },\n",
    "  {\n",
    "    \"invalid\":\"invalid\"\n",
    "  }  \n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# JSON file creation\n",
    "import os\n",
    "file_path = \"/home/hduser/employe_json/sample.json\"\n",
    "directory = os.path.dirname(file_path)\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "with open(\"/home/hduser/employe_json/sample.json\", \"w\") as f:\n",
    "    f.write(samplejson)\n",
    "\n",
    "# Define a custom schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", DecimalType(10, 3), True),\n",
    "    StructField(\"isActive\", BooleanType(), True),\n",
    "    StructField(\"comments\", StringType(), True),\n",
    "    StructField(\"tags\", ArrayType(StringType()), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"street\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True)\n",
    "    ])),\n",
    "    StructField(\"date_joined\", DateType(), True),\n",
    "    StructField(\"timestamp_event\", TimestampType(), True),\n",
    "    StructField(\"corrupted_record\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Read JSON with multiple options and inline comments\n",
    "df = spark.read.json(\n",
    "    path=\"file:///home/hduser/employe_json/\",  # Specifies the location of the JSON file(s) to read.\n",
    "    schema=custom_schema,  # Defines a custom schema to avoid automatic inference.\n",
    "    primitivesAsString=False,  # Treats all primitive values (int, float, bool) as strings if True.\n",
    "    prefersDecimal=True,  # Infers floating-point numbers as DecimalType instead of DoubleType.\n",
    "    allowComments=True,  # Allows Java/C++ style comments (//, /* */) in JSON.\n",
    "    allowUnquotedFieldNames=True,  # Accepts JSON keys without double quotes.\n",
    "    allowSingleQuotes=True,  # Accepts single quotes for string values.\n",
    "    allowBackslashEscapingAnyCharacter=True,  # Allows any character to be escaped with a backslash.\n",
    "    mode=\"PERMISSIVE\",  # Defines how to handle corrupt records (PERMISSIVE, DROPMALFORMED, FAILFAST).\n",
    "    columnNameOfCorruptRecord=\"corrupted_record\",  # Stores malformed JSON strings in a specified column.\n",
    "    dateFormat=\"yyyy-MM-dd\",  # Specifies the format for parsing date strings.\n",
    "    timestampFormat=\"yyyy-MM-dd HH:mm:ss.SSS\",  # Specifies the format for parsing timestamp strings.\n",
    "    multiLine=True,  # Treats the entire file as a single JSON object (for pretty-printed or array JSON).\n",
    "    allowUnquotedControlChars=True,  # Allows control characters (e.g., \\n, \\t) to appear unquoted.\n",
    "    lineSep=\"\\n\",  # Defines a custom line separator between JSON records. If multiline=True then it is not required.\n",
    "    samplingRatio=1.0,  # Sets the fraction of data used for schema inference.\n",
    "    encoding=\"UTF-8\",  # Specifies the character encoding (e.g., UTF-8, UTF-16).\n",
    "    locale=\"en-US\",  # Sets the locale for parsing locale-sensitive data like dates.\n",
    "    pathGlobFilter=\"*.json\",  # Filters files using glob patterns (e.g., *.json).\n",
    "    recursiveFileLookup=True  # Enables recursive search in subdirectories.\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645d216-3f64-40c3-838e-d9a9c79372db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **5. Reading a CSV data with various options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "315837f5-9161-4ae0-866a-453314f0feaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "|symbol                           |exchange|date      |timestamp          |price   |volume |corrupted_data                                     |\n",
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "|AAPL                             |NYSE    |2023-08-01|2023-08-01 09:30:00|195.25  |1200000|NULL                                               |\n",
      "|GOOGL                            |NYSE    |2023-08-01|2023-08-01 09:30:00|2735.55 |850000 |NULL                                               |\n",
      "|MSFT                             |NYSE    |2023-08-01|2023-08-01 09:30:00|NaN     |950000 |NULL                                               |\n",
      "|TSLA                             |NYSE    |2023-08-01|2023-08-01 09:30:00|Infinity|1100000|NULL                                               |\n",
      "|AMZN                             |NYSE    |NULL      |2023-08-01 09:30:00|134.25  |NULL   |NULL                                               |\n",
      "|MSFK                             |NYSE    |2023-08-01|NULL               |100.01  |950000 |MSFK,NYSE,2023-08-01,2023-08-01 09:30,100.01,950000|\n",
      "|INVALID_ROW_WITHOUT_PROPER_FIELDS|NULL    |NULL      |NULL               |NULL    |NULL   |INVALID_ROW_WITHOUT_PROPER_FIELDS                  |\n",
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "\n",
      "[INFO] Corruputed Rows\n",
      "+---------------------------------+--------+----------+---------+------+------+---------------------------------------------------+\n",
      "|symbol                           |exchange|date      |timestamp|price |volume|corrupted_data                                     |\n",
      "+---------------------------------+--------+----------+---------+------+------+---------------------------------------------------+\n",
      "|MSFK                             |NYSE    |2023-08-01|NULL     |100.01|950000|MSFK,NYSE,2023-08-01,2023-08-01 09:30,100.01,950000|\n",
      "|INVALID_ROW_WITHOUT_PROPER_FIELDS|NULL    |NULL      |NULL     |NULL  |NULL  |INVALID_ROW_WITHOUT_PROPER_FIELDS                  |\n",
      "+---------------------------------+--------+----------+---------+------+------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Data\n",
    "data = \"\"\"\n",
    "symbol,exchange,date,timestamp,price,volume\n",
    "AAPL,NYSE,2023-08-01,2023-08-01 09:30:00,195.25,1200000\n",
    "GOOGL,NYSE,2023-08-01,2023-08-01 09:30:00,2735.55,850000\n",
    "MSFT,NYSE,2023-08-01,2023-08-01 09:30:00,-1,950000\n",
    "TSLA,NYSE,2023-08-01,2023-08-01 09:30:00,Inf,1100000\n",
    "AMZN,NYSE,na,2023-08-01 09:30:00,134.25,na\n",
    "MSFK,NYSE,2023-08-01,2023-08-01 09:30,100.01,950000\n",
    "INVALID_ROW_WITHOUT_PROPER_FIELDS       \n",
    "\"\"\"\n",
    "\n",
    "# CSV file creation\n",
    "file_path = \"/home/hduser/employe_csv/sample.csv\"\n",
    "directory = os.path.dirname(file_path)\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(data)\n",
    "\n",
    "# Define custom schema\n",
    "customschema = StructType([\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"exchange\", StringType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"volume\", IntegerType(), True),\n",
    "    StructField(\"corrupted_data\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read CSV with various options\n",
    "df1 = spark.read.csv(\n",
    "    path=\"file:///home/hduser/employe_csv/sample.csv\",\n",
    "    sep=',',  # Column separator used in the CSV file\n",
    "    header=True,  # First line of the file contains column headers\n",
    "    schema=customschema,  # Custom schema to define data types and structure\n",
    "    columnNameOfCorruptRecord='corrupted_data',  # Stores malformed rows in this column\n",
    "    encoding='UTF-8',  # Character encoding used to read the file\n",
    "    quote=\"'\",  # Defines single quote as the string quoting character\n",
    "    comment='-',  # Lines starting with '-' are treated as comments and ignored\n",
    "    ignoreTrailingWhiteSpace=True,  # Trims trailing whitespace from fields\n",
    "    ignoreLeadingWhiteSpace=True,  # Trims leading whitespace from fields\n",
    "    nullValue='na',  # Treats 'na' as a null value\n",
    "    nanValue='-1',  # Treats '-1' as NaN (Not a Number)\n",
    "    positiveInf='Inf',  # Treats 'Inf' as positive infinity\n",
    "    dateFormat='yyyy-MM-dd',  # Format used to parse date fields\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',  # Format used to parse timestamp fields\n",
    "    maxColumns=40  # Maximum number of columns allowed in the file\n",
    ")\n",
    "\n",
    "# Show first 10 rows\n",
    "df1.show(10, False)\n",
    "\n",
    "print(\"[INFO] Corruputed Rows\")\n",
    "# Cache and filter corrupted rows\n",
    "df2 = df1.cache().where(\"corrupted_data is not null\")\n",
    "df2.show(10, False)  # Display malformed rows for RCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70210465-05af-4071-b1d4-83e0296c48b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **6. ORC & Parquet file format for Performance Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fad88c1-78cc-4719-b956-7984dbdcec60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Scrubbed Data\n",
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "|symbol                           |exchange|date      |timestamp          |price   |volume |corrupted_data                                     |\n",
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "|AAPL                             |NYSE    |2023-08-01|2023-08-01 09:30:00|195.25  |1200000|NULL                                               |\n",
      "|GOOGL                            |NYSE    |2023-08-01|2023-08-01 09:30:00|2735.55 |850000 |NULL                                               |\n",
      "|MSFT                             |NYSE    |2023-08-01|2023-08-01 09:30:00|NaN     |950000 |NULL                                               |\n",
      "|TSLA                             |NYSE    |2023-08-01|2023-08-01 09:30:00|Infinity|1100000|NULL                                               |\n",
      "|AMZN                             |NYSE    |NULL      |2023-08-01 09:30:00|134.25  |NULL   |NULL                                               |\n",
      "|MSFK                             |NYSE    |2023-08-01|NULL               |100.01  |950000 |MSFK,NYSE,2023-08-01,2023-08-01 09:30,100.01,950000|\n",
      "|INVALID_ROW_WITHOUT_PROPER_FIELDS|NULL    |NULL      |NULL               |NULL    |NULL   |INVALID_ROW_WITHOUT_PROPER_FIELDS                  |\n",
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "\n",
      "[INFO] Cureated Data\n",
      "+------+--------+----------+-------------------+--------+-------+--------------+\n",
      "|symbol|exchange|date      |timestamp          |price   |volume |corrupted_data|\n",
      "+------+--------+----------+-------------------+--------+-------+--------------+\n",
      "|AAPL  |NYSE    |2023-08-01|2023-08-01 09:30:00|195.25  |1200000|NULL          |\n",
      "|GOOGL |NYSE    |2023-08-01|2023-08-01 09:30:00|2735.55 |850000 |NULL          |\n",
      "|MSFT  |NYSE    |2023-08-01|2023-08-01 09:30:00|NaN     |950000 |NULL          |\n",
      "|TSLA  |NYSE    |2023-08-01|2023-08-01 09:30:00|Infinity|1100000|NULL          |\n",
      "|AMZN  |NYSE    |NULL      |2023-08-01 09:30:00|134.25  |NULL   |NULL          |\n",
      "+------+--------+----------+-------------------+--------+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 21:45:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stock_symbol, exchange, date, timestamp, price, volume\n",
      " Schema: symbol, exchange, date, timestamp, price, volume\n",
      "Expected: symbol but found: stock_symbol\n",
      "CSV file: file:///tmp/nyse_header_options2.csv\n",
      "25/08/05 21:45:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stock_symbol, exchange, date, timestamp, price, volume\n",
      " Schema: symbol, exchange, date, timestamp, price, volume\n",
      "Expected: symbol but found: stock_symbol\n",
      "CSV file: file:///tmp/nyse_header_options2.csv\n"
     ]
    }
   ],
   "source": [
    "# Data SCHEMA MIGRATION from csv (Struct) to ORC/Parquet (serialized-binary) and load into our DATALAKE\n",
    "\n",
    "# Data\n",
    "data =\"\"\" \n",
    "stock_symbol,exchange,date,timestamp,price,volume\n",
    "AAPL,NYSE,2023-08-01,2023-08-01 09:30:00,195.25,1200000\n",
    "GOOGL,NYSE,2023-08-01,2023-08-01 09:30:00,2735.55,850000\n",
    "MSFT,NYSE,2023-08-01,2023-08-01 09:30:00,-1,950000\n",
    "TSLA,NYSE,2023-08-01,2023-08-01 09:30:00,Inf,1100000\n",
    "AMZN,NYSE,na,2023-08-01 09:30:00,134.25,na\n",
    "MSFK,NYSE,2023-08-01,2023-08-01 09:30,100.01,950000\n",
    "INVALID_ROW_WITHOUT_PROPER_FIELDS \n",
    "\"\"\"\n",
    " \n",
    "with open(\"/tmp/nyse_header_options2.csv\", \"w\") as f:\n",
    "    f.write(data)\n",
    "  \n",
    "# Define custom schema\n",
    "customschema = StructType([\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"exchange\", StringType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"volume\", IntegerType(), True),\n",
    "    StructField(\"corrupted_data\", StringType(), True)\n",
    "])\n",
    " \n",
    "# Read CSV with various options\n",
    "df1 = spark.read.csv(\n",
    "    'file:///tmp/nyse_header_options2.csv',\n",
    "    sep=',',  # Column separator used in the CSV file\n",
    "    header=True,  # First line of the file contains column headers\n",
    "    schema=customschema,  # Custom schema to define data types and structure\n",
    "    columnNameOfCorruptRecord='corrupted_data',  # Stores malformed rows in this column\n",
    "    encoding='UTF-8',  # Character encoding used to read the file\n",
    "    quote=\"'\",  # Defines single quote as the string quoting character\n",
    "    comment='-',  # Lines starting with '-' are treated as comments and ignored\n",
    "    ignoreTrailingWhiteSpace=True,  # Trims trailing whitespace from fields\n",
    "    ignoreLeadingWhiteSpace=True,  # Trims leading whitespace from fields\n",
    "    nullValue='na',  # Treats 'na' as a null value\n",
    "    nanValue='-1',  # Treats '-1' as NaN (Not a Number)\n",
    "    positiveInf='Inf',  # Treats 'Inf' as positive infinity\n",
    "    dateFormat='yyyy-MM-dd',  # Format used to parse date fields\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',  # Format used to parse timestamp fields\n",
    "    maxColumns=40  # Maximum number of columns allowed in the file\n",
    " \n",
    ")\n",
    " \n",
    "# Show first 10 rows\n",
    "print(\"[INFO] Scrubbed Data\")\n",
    "df1.show(10, False)\n",
    " \n",
    "# Cache and filter corrupted rows\n",
    "df2 = df1.cache().where(\"corrupted_data is null\")\n",
    "print(\"[INFO] Cureated Data\")\n",
    "df2.show(10, False)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a0dc04d-e251-4bc2-8219-629152ccbf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading data from ORC\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|symbol|exchange|timestamp          |price   |volume |corrupted_data|date      |\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|GOOGL |NYSE    |2023-08-01 09:30:00|2735.55 |850000 |NULL          |2023-08-01|\n",
      "|MSFT  |NYSE    |2023-08-01 09:30:00|NaN     |950000 |NULL          |2023-08-01|\n",
      "|TSLA  |NYSE    |2023-08-01 09:30:00|Infinity|1100000|NULL          |2023-08-01|\n",
      "|AAPL  |NYSE    |2023-08-01 09:30:00|195.25  |1200000|NULL          |2023-08-01|\n",
      "|AMZN  |NYSE    |2023-08-01 09:30:00|134.25  |NULL   |NULL          |NULL      |\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "\n",
      "[INFO] Reading data from ORC using SQL\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|symbol|exchange|timestamp          |price   |volume |corrupted_data|date      |\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|GOOGL |NYSE    |2023-08-01 09:30:00|2735.55 |850000 |NULL          |2023-08-01|\n",
      "|MSFT  |NYSE    |2023-08-01 09:30:00|NaN     |950000 |NULL          |2023-08-01|\n",
      "|TSLA  |NYSE    |2023-08-01 09:30:00|Infinity|1100000|NULL          |2023-08-01|\n",
      "|AAPL  |NYSE    |2023-08-01 09:30:00|195.25  |1200000|NULL          |2023-08-01|\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data SCHEMA MIGRATION from csv (Struct) to ORC (serialized-binary) and load into our DATALAKE\n",
    "# Write the clean data in ORC format for internal teams data querying furpose\n",
    "df2.write.orc('file:///tmp/stocks_orc',mode='overwrite') #(Datalake) hdfs:///user/hduser/custorcout\n",
    "df2.write.orc('file:///tmp/stocks_orc_lzo',mode='ignore',compression='lzo')\n",
    "df2.write.orc('file:///tmp/stocks_orc_lzo_part',mode='overwrite',partitionBy='date')\n",
    "\n",
    "# Reading the data from ORC\n",
    "df_orc = spark.read.orc(\"file:///tmp/stocks_orc_lzo_part\")\n",
    "print(\"[INFO] Reading data from ORC\")\n",
    "df_orc.show(truncate=False)\n",
    " \n",
    "df_orc_sql = spark.sql(\"select * from orc.`file:///tmp/stocks_orc_lzo_part` where date is not null\")\n",
    "print(\"[INFO] Reading data from ORC using SQL\")\n",
    "df_orc_sql.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f15171f8-9fda-42cf-8362-24d63e1db2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading data from Parquet\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|symbol|exchange|timestamp          |price   |volume |corrupted_data|date      |\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|GOOGL |NYSE    |2023-08-01 09:30:00|2735.55 |850000 |NULL          |2023-08-01|\n",
      "|MSFT  |NYSE    |2023-08-01 09:30:00|NaN     |950000 |NULL          |2023-08-01|\n",
      "|TSLA  |NYSE    |2023-08-01 09:30:00|Infinity|1100000|NULL          |2023-08-01|\n",
      "|AAPL  |NYSE    |2023-08-01 09:30:00|195.25  |1200000|NULL          |2023-08-01|\n",
      "|AMZN  |NYSE    |2023-08-01 09:30:00|134.25  |NULL   |NULL          |NULL      |\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "\n",
      "[INFO] Reading data from Parquet using SQL\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|symbol|exchange|timestamp          |price   |volume |corrupted_data|date      |\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|GOOGL |NYSE    |2023-08-01 09:30:00|2735.55 |850000 |NULL          |2023-08-01|\n",
      "|MSFT  |NYSE    |2023-08-01 09:30:00|NaN     |950000 |NULL          |2023-08-01|\n",
      "|TSLA  |NYSE    |2023-08-01 09:30:00|Infinity|1100000|NULL          |2023-08-01|\n",
      "|AAPL  |NYSE    |2023-08-01 09:30:00|195.25  |1200000|NULL          |2023-08-01|\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data SCHEMA MIGRATION from csv (Struct) to PARQUET (serialized-binary) and load into our DATALAKE\n",
    "# Write the clean data in ORC format for internal teams data querying furpose\n",
    "df2.write.parquet('file:///tmp/stocks_parquet',mode='overwrite') #(Datalake) hdfs:///user/hduser/custorcout\n",
    "df2.write.parquet('file:///tmp/stocks_parquet_lzo',mode='ignore',compression='snappy')\n",
    "df2.write.parquet('file:///tmp/stocks_parquet_part',mode='overwrite',partitionBy='date')\n",
    " \n",
    "# Reading the data from Parquet\n",
    "df_orc = spark.read.parquet(\"file:///tmp/stocks_parquet_part\")\n",
    "print(\"[INFO] Reading data from Parquet\")\n",
    "df_orc.show(truncate=False)\n",
    " \n",
    "df_orc_sql = spark.sql(\"select * from parquet.`file:///tmp/stocks_parquet_part` where date is not null\")\n",
    "print(\"[INFO] Reading data from Parquet using SQL\")\n",
    "df_orc_sql.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f06c24-b9e8-4e35-9157-5c1d6eaf2dc8",
   "metadata": {},
   "source": [
    "## **7. PySpark and Hive Integration : Data Ingestion and Table Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b6b6c98-87c0-4d27-9455-1e9268612cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script demonstrates various methods for writing data from a PySpark\n",
    "# DataFrame into Hive tables, highlighting best practices, limitations,\n",
    "# and common use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89c26bec-f531-4609-bcd6-fdf978c483c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+---+--------------------+\n",
      "|    cid|   fname|     lname|age|          profession|\n",
      "+-------+--------+----------+---+--------------------+\n",
      "|4000001|Kristina|     Chung| 55|               Pilot|\n",
      "|4000002|   Paige|      Chen| 77|             Teacher|\n",
      "|4000003|  Sherri|    Melton| 34|         Firefighter|\n",
      "|4000004|Gretchen|      Hill| 66|Computer hardware...|\n",
      "|4000005|   Karen|   Puckett| 74|              Lawyer|\n",
      "|4000006| Patrick|      Song| 42|        Veterinarian|\n",
      "|4000007|   Elsie|  Hamilton| 43|               Pilot|\n",
      "|4000008|   Hazel|    Bender| 63|           Carpenter|\n",
      "|4000009| Malcolm|    Wagner| 39|              Artist|\n",
      "|4000010| Dolores|McLaughlin| 60|              Writer|\n",
      "|4000011| Francis|  McNamara| 47|           Therapist|\n",
      "|4000012|   Sandy|    Raynor| 26|              Writer|\n",
      "|4000013|  Marion|      Moon| 41|           Carpenter|\n",
      "|4000014|    Beth|   Woodard| 65|                NULL|\n",
      "|4000015|   Julia|     Desai| 49|            Musician|\n",
      "|4000016|  Jerome|   Wallace| 52|          Pharmacist|\n",
      "|4000017|    Neal|  Lawrence| 72|Computer support ...|\n",
      "|4000018|    Jean|   Griffin| 45|    Childcare worker|\n",
      "|4000019|Kristine| Dougherty| 63|   Financial analyst|\n",
      "|4000020| Crystal|    Powers| 67|Engineering techn...|\n",
      "+-------+--------+----------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ShortType\n",
    "\n",
    "# Placeholder for the data and schema\n",
    "cust_schema = StructType([\n",
    "    StructField('cid', IntegerType(), nullable=False),\n",
    "    StructField('fname', StringType()),\n",
    "    StructField('lname', StringType()),\n",
    "    StructField('age', ShortType()),\n",
    "    StructField('profession', StringType())\n",
    "])\n",
    "\n",
    "df1 = spark.read.csv(\n",
    "     'file:///home/hduser/custinfo.csv',\n",
    "     schema=cust_schema,\n",
    "     header=False,\n",
    "     sep=',',\n",
    "     mode='dropmalformed'\n",
    ")\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd840dc-69c8-4fb9-bd21-bf60df853dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Method 2: Using the `insertInto` function\n",
    "#\n",
    "# This method loads data into an existing table. It's less common for initial\n",
    "# table creation and data loading, as it does not create a new table schema.\n",
    "# ==============================================================================\n",
    "print(\"--- Method 2: Inserting data into an existing table using insertInto ---\")\n",
    "# This requires the 'default.customers' table to already exist.\n",
    "# The schema of the DataFrame must match the table schema.\n",
    "# df1.write.insertInto('wholesale.customers', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d09ba3-5b41-4c9b-8a09-ac3ce6e0d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Method 3: Storing as CSV (PySpark only)\n",
    "#\n",
    "# This method creates a table with data stored in CSV format. This table is\n",
    "# typically only accessible and readable via PySpark, not directly via HiveQL.\n",
    "# The SerDe (Serializer/Deserializer) for Spark-written CSVs is not\n",
    "# compatible with Hive's default TextFile SerDe.\n",
    "# ==============================================================================\n",
    "print(\"--- Method 3: Creating a CSV table (PySpark-only access) ---\")\n",
    "# Creates a managed table with data stored as CSV files.\n",
    "# Hive CLI will not be able to read this table correctly.\n",
    "df1.write.saveAsTable(\n",
    "    'default.customers_csv',\n",
    "    format='csv',\n",
    "    sep=',',\n",
    "    mode='overwrite'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fffd58-edbd-4751-a65d-3b1870699562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Method 4: Using Hive Literal Syntax\n",
    "#\n",
    "# This approach uses direct HiveQL statements to create a table that is\n",
    "# fully compatible with both Hive and PySpark. It is the proper way to\n",
    "# meet a requirement for a TextFile table with a specific delimiter.\n",
    "# ==============================================================================\n",
    "print(\"--- Method 4: Creating a TextFile table using HiveQL (interoperable) ---\")\n",
    "# Step 1: Create the table using HiveQL with the specified row format.\n",
    "spark.sql(\"\"\"\n",
    "   CREATE TABLE default.customers_text (\n",
    "        id INT,\n",
    "        fname STRING,\n",
    "        lname STRING,\n",
    "        age INT,\n",
    "        prof STRING\n",
    "    )\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    STORED AS TEXTFILE\n",
    "\"\"\")\n",
    "\n",
    "# Step 2: Load the data into the newly created table.\n",
    "# Note: 'local' means the file is on the driver's local filesystem.\n",
    "\n",
    "spark.sql(\n",
    "    \"LOAD DATA LOCAL INPATH 'file:///home/hduser/custinfo.csv' OVERWRITE INTO TABLE default.customers_text\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90269c1-a89a-4f79-9965-535d7e57ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Method 5: Marrying DataFrame to a Hive Table using a View and Insert Select\n",
    "#\n",
    "# This is a common pattern to load data from a DataFrame into an existing\n",
    "# Hive table, providing full interoperability.\n",
    "# ==============================================================================\n",
    "print(\"--- Method 5: Using Insert Select from a temporary view ---\")\n",
    "# Step 1: Create a temporary view from the DataFrame.\n",
    "df1.createOrReplaceTempView(\"view1\")\n",
    "\n",
    "# Step 2: Insert data from the view into an existing Hive table.\n",
    "# Assumes 'default.customers_text' exists and has a compatible schema.\n",
    "spark.sql(\"INSERT OVERWRITE TABLE default.customers_csv SELECT * FROM view1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ddb6aa-0422-44ff-a193-4ce50cf934b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Creating External Tables\n",
    "#\n",
    "# Demonstrates creating external tables, where the data is not managed by Hive,\n",
    "# using both non-partitioned and partitioned approaches.\n",
    "# ==============================================================================\n",
    "print(\"--- Creating a non-partitioned external table ---\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE default.customers_text_ext (\n",
    "        id INT,\n",
    "        fname STRING,\n",
    "        lname STRING,\n",
    "        age INT,\n",
    "        prof STRING\n",
    "    )\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    STORED AS TEXTFILE \n",
    "    LOCATION '/user/hduser/customer_ext_table'\n",
    "\"\"\")\n",
    "# Loading data into the external table.\n",
    "spark.sql(\n",
    "    \"LOAD DATA LOCAL INPATH 'file:///home/hduser/custinfo.csv' OVERWRITE INTO TABLE default.customers_text_ext\"\n",
    ")\n",
    "\n",
    "print(\"--- Creating a partitioned external table ---\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE default.customers_text_ext_part (\n",
    "        id INT,\n",
    "        fname STRING,\n",
    "        lname STRING,\n",
    "        age INT\n",
    "    )\n",
    "    PARTITIONED BY (prof STRING)\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    STORED AS TEXTFILE \n",
    "    LOCATION '/user/hduser/customer_ext_table_part'\n",
    "\"\"\")\n",
    "\n",
    "# Note: `LOAD DATA` does not support dynamic partitioning.\n",
    "# Instead, we use `INSERT OVERWRITE` with dynamic partitioning enabled.\n",
    "spark.sql(\"SET spark.sql.sources.partitionOverwriteMode=dynamic\") # Use Spark setting for partition overwrite mode\n",
    "spark.sql(\"SET hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "\n",
    "# Insert data dynamically into the partitioned table.\n",
    "spark.sql(\n",
    "    \"INSERT OVERWRITE TABLE default.customers_text_ext_part PARTITION(prof) SELECT id, fname, lname, age, prof FROM view1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ad275-4356-4798-a753-13198e9eae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Spark and Hive Bucketing Incompatibility\n",
    "#\n",
    "# This is a critical point about a known limitation. Spark and Hive use\n",
    "# different hashing algorithms for bucketing, making them incompatible.\n",
    "# ==============================================================================\n",
    "print(\"--- Demonstrating Spark-Hive Bucketing Incompatibility ---\")\n",
    "# Drop the table if it already exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS default.customers_text_ext_part_bucket\")\n",
    "\n",
    "# Create a bucketed table using HiveQL\n",
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE default.customers_text_ext_part_bucket (\n",
    "        id INT,\n",
    "        fname STRING,\n",
    "        lname STRING,\n",
    "        age INT,\n",
    "        prof STRING\n",
    "    )\n",
    "    CLUSTERED BY (id) INTO 10 BUCKETS\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    STORED AS TEXTFILE \n",
    "    LOCATION '/user/hduser/customer_ext_table_part_bucket'\n",
    "\"\"\")\n",
    "\n",
    "# Attempting to load data from a DataFrame into this table will fail\n",
    "# because Spark's bucketing algorithm is not compatible with Hive's.\n",
    "# The following line will raise an AnalysisException.\n",
    "# spark.sql(\"INSERT OVERWRITE TABLE wholesale.customers_text_ext_part_bucket SELECT * FROM view1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922e125-8914-4457-a93b-c72477637d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Solution for High Performance: Combining Partitioning, Bucketing, and Columnar Format\n",
    "#\n",
    "# This code shows how to create a highly optimized table entirely within\n",
    "# PySpark, which is best for Spark-native queries.\n",
    "# Default format is Parquet to saving the data and Snappy codec for compression.\n",
    "# ==============================================================================\n",
    "print(\"--- Creating a highly performant table using PySpark native methods ---\")\n",
    "df1.write.bucketBy(10, 'cid').sortBy(\"cid\").\\\n",
    "    saveAsTable(\n",
    "        'default.customers_part_buck_parquet_snappy',\n",
    "        mode='overwrite',\n",
    "        partitionBy='profession'\n",
    "    )\n",
    "\n",
    "print(\"--- Conclusion on PySpark and Hive Integration ---\")\n",
    "print(\"PySpark and Hive have strong integration, but it's crucial to understand\")\n",
    "print(\"the differences in their internal implementations, especially concerning\")\n",
    "print(\"file formats and bucketing algorithms, to ensure interoperability and performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f769c9-0b34-4144-b601-f93e35c68cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
