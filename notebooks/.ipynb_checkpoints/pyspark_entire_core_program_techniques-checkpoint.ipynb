{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "806df89d-116d-4b3b-925c-f5f2f9f866ed",
   "metadata": {},
   "source": [
    "# **PySpark Core Program Techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cc9a4fa-f8bb-4439-863a-490f3cf38cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from pyspark.sql.session import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48137615-5a5a-4ee9-be4b-9505b4577774",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **1. How to create SparkSession object creation & sparkContext reference?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7c057f0-c814-4bf9-93a7-72a8db753e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SparkSession is the entry point for accessing the mypyspark cluster operation.\n",
    "SparkSession object will instantiate SparkContex, SqlContext, HiveContext Objects.\n",
    "|__SparkSession is a Class\n",
    "      |__builder is a Class variable/object/attribute to call the SparkSession class methods like master(), appName(), enableHiveSupport(), getOrCreate()\n",
    "           |__master('yarn')           => method to help us submit the mypyspark application to the respective cluster manager\n",
    "           |__appName('program-name')  => mypyspark program name that help us identify the jobs runs in a cluster   \n",
    "           |__enableHiveSupport()      => HiveQueryLanguage(HQL) method help us to create Catalog, UDFs, etc.,\n",
    "           |__getOrCreate()            => method to create a new SparkSession object or referring to existing SparkSession\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f255926-c2a9-4c92-9113-18b2afab6620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0xffff5008f5e0>\n",
      "SparkContext: <SparkContext master=local[*] appName=pyspark-shell>\n",
      "SparkContext: <pyspark.sql.context.SQLContext object at 0xffff44a72280>\n",
      "HiveContext: <pyspark.sql.context.HiveContext object at 0xffff44bdff70>\n"
     ]
    }
   ],
   "source": [
    "#SparkSession Object Creation\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "print(spark)\n",
    "\n",
    "#Accessing the underlying SparkContext with the Spark Session\n",
    "sc=spark.sparkContext\n",
    "print(f\"SparkContext:\",sc)\n",
    "\n",
    "#Accessing the underlying SQLContext with the Spark Session\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "print(f\"SparkContext:\",sqlContext)\n",
    "\n",
    "#Accessing the underlying HiveContext with the Spark Session\n",
    "from pyspark.sql import HiveContext\n",
    "hiveContext = HiveContext(sc)\n",
    "print(f\"HiveContext:\",hiveContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c70e8-aed1-4929-8059-10cf177089ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1.  Can we have more than SparkContext in a same application? \n",
    "        No, only one can be created. Also it is not allowd and not a good practice to load same program in different memory containers again and again.\n",
    "        Other wise we will get the error: sc=mypyspark.sparkContext() TypeError: 'SparkContext' object is not callable\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcf8e15b-7ced-4a2f-b0cd-d2920bd7363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception Occured: 'SparkContext' object is not callable\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc1=spark.sparkContext()\n",
    "    print(f\"SparkContext:\",sc1)\n",
    "except Exception as e:\n",
    "    print(f\"Exception Occured: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b7df9-325f-4ba2-9c3b-0ed3a9610292",
   "metadata": {},
   "source": [
    "## **2. RDD (Resilient Distributed Dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08bfb77-5d9f-4ea0-a3b8-1668cf03a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1. Spark Terminologies?\n",
    "    1. RDD            :Resilient(can be rebuild) Distribuited(across multiple nodes memory) Dataset (can come from anywhere)\n",
    "    2. DAG            :(Direct Acyclic Graph)\n",
    "    3. Transformation :\n",
    "    4. Action         :\n",
    "    5. Lineage        :(Direct relation between transformation and action)\n",
    "    \n",
    "Q2. What is RDD?\n",
    "        Resilient Distributed Dataset, Lazily evaluated and executed, Immutable, Core Spark Abstraction, Fundamental unit of data, Lineage to rebuild.\n",
    "    \n",
    "Q3. What are ways to create RDDs?\n",
    "        1. RDD from any sources(different filesystems)\n",
    "        2. RDDs/DFs can be created programatically\n",
    "        3. RDDs/DFs from another RDD/DF\n",
    "        4. RDD/DF from memory \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63954478-23a8-4657-8b34-6bd188165bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "download the dataset \"custinfo.csv\" to linux and hadoop home folder:\n",
    "\n",
    "ls /home/hduser/custinfo.csv\n",
    "\n",
    "hadoop fs -put /home/hduser/custinfo.csv /user/hduser/\n",
    "hadoop fs -ls /user/hduser/custinfor.csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08ce7b75-c674-483f-bab9-ba34daf623d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. RDD creation from any sources (different filesystems)\n",
    "file_rdd1 = sc.textFile(\"/home/hduser/custinfo.csv\") #linux file system\n",
    "hdfs_rdd1 = sc.textFile(\"/user/hduser/custinfo.csv\") #hdfs file system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1da35a9d-152a-4360-bb91-b3990cb0ad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salary_list_rdd1.collect()=>[20000, 30000, 15000, 40000, 50000]\n",
      "salary_list_rdd1.glom().collect()=>[[20000, 30000], [15000, 40000, 50000]]\n"
     ]
    }
   ],
   "source": [
    "#2. RDD creation programatically\n",
    "program_rdd1 =  sc.parallelize(range(1,1000))\n",
    "\n",
    "salary_list = [20000,30000,15000,40000,50000]\n",
    "salary_list_rdd1 = sc.parallelize(salary_list,2) #Creating distributed RDD referencing 2 memory location (partitions)\n",
    "print(f\"salary_list_rdd1.collect()=>{salary_list_rdd1.collect()}\") #Collect Action: Consolidate all the partitions and produce one result\n",
    "print(f\"salary_list_rdd1.glom().collect()=>{salary_list_rdd1.glom().collect()}\") # Collect Action: Partition wise collect output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "def0aa9e-5f8b-48de-a6c2-86d77e40e598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd2.collect()=>[21000, 31000, 16000, 41000, 51000]\n",
      "rdd2.glom().collect()=>[[21000, 31000], [16000, 41000, 51000]]\n"
     ]
    }
   ],
   "source": [
    "#Typical Spark Core Application like below\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "rdd1 = sc.parallelize([20000,30000,15000,40000,50000],2)\n",
    "rdd2 = rdd1.map(lambda sal:sal+1000) # Transformation (Map - no element count is not changed)\n",
    "print(f\"rdd2.collect()=>{rdd2.collect()}\")\n",
    "print(f\"rdd2.glom().collect()=>{rdd2.glom().collect()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2092b178-aa35-4ed6-9ca2-668bb1ebd2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. RDDs/DFs from another RDD/DF\n",
    "rdd2 = rdd1.map(lambda sal:sal+1000) #rdd2 created from rdd1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24e83674-d8b3-4d70-8f66-1f70c994cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. RDD/DF from memory\n",
    "rdd1.cache()                         #rdd1 value will be persist in the memory till the program complete exit\n",
    "rdd2 = rdd1.map(lambda sal:sal+1000) #rdd2 is created from memory RDD(rdd1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a709f-6f05-4089-a7bd-7325798912e0",
   "metadata": {},
   "source": [
    "## **3. What is Transformation & Action in RDD?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04eb62-e674-4913-8f52-e65483dba8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1. What is Transformation & Action in RDD?\n",
    "        Transformation:  If a function/method returns another RDD.   Operations => map(), flatMap(), filter(), distinct(), union()\n",
    "        Action        :  If a function/method returns RESULT(VALUE). Operations => collect(), count(), take(3), reduce(), saveAsTextFile()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40feb8ce-fbde-44cf-bee1-b3e1938fcc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([20000,30000,15000,40000,50000],2)\n",
    "rdd2 = rdd1.map(lambda sal:sal+1000) #Transformation (MAP returns another RDD)\n",
    "print(rdd2.count())                  #Action (COUNT trigers computation and returns RESULT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a86b322-05ed-404a-97df-760e4c9b61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q2. What are types of Transformation?\n",
    "        Active:  If the output number of elements of a given RDD is different from the input number of element of an RDD.\n",
    "        Passive: If the output number of elements of a given RDD is same      from the input number of element of an RDD.\n",
    "\n",
    "#/home/hduser/custinfo.csv\n",
    "4000001,Kristina,Chung,55,Pilot\n",
    "4000002,Paige,Chen,77,Teacher\n",
    "4000003,Sherri,Melton,34,Firefighter\n",
    "4000004,Gretchen,Hill,66,Computer hardware engineer\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
