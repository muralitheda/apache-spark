{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d55ae10-939c-4905-8f70-7f2a73350867",
   "metadata": {},
   "source": [
    "# **PySpark Ingestion + Egress + Dataloading Techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7fa8ec8-e8f6-4c26-a269-20bad66f4c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SparkSession Object Memory Reference: <pyspark.sql.session.SparkSession object at 0xffff4557f3d0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#MySql jdbc connector jar local path\n",
    "mysql_connector_jar_path = \"/home/hduser/install/mysql-connector-java.jar\"\n",
    "\n",
    "#Spark Session Creation\n",
    "spark =  SparkSession.builder\\\n",
    "    .appName(\"Spark-Ingress-Egress-Dataloading-Practice\")\\\n",
    "    .config(\"spark.jars\", mysql_connector_jar_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"[INFO] SparkSession Object Memory Reference: {spark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54905feb-da58-4961-a550-9f7dd7eb23d5",
   "metadata": {},
   "source": [
    "## **1. Converting Unstructured/Semi Structured Data to Structured Data using RDD then to DF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5421c0a6-2025-44de-beb7-57e26fe2ab29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|distinct_courses|\n",
      "+----------------+\n",
      "|            Rust|\n",
      "|          Docker|\n",
      "|      JavaScript|\n",
      "|         PyTorch|\n",
      "|             AWS|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-----------------------+\n",
      "|    cource|no_of_people_interested|\n",
      "+----------+-----------------------+\n",
      "|      Rust|                 104000|\n",
      "|    Docker|                 104000|\n",
      "|JavaScript|                  52000|\n",
      "|   PyTorch|                  52000|\n",
      "|       AWS|                  52000|\n",
      "+----------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Case1: Users entered course data in Unstructured format. We would like to know the distinct number of course, which course mostly wanted?\n",
    "\n",
    "#Sample data => samplecourse.log\n",
    "\"\"\"\n",
    "Docker Java JavaScript React Spark Kafka SQL Git Go Python AI\n",
    "\n",
    "AWS Azure TensorFlow PyTorch Android iOS Rust AI\n",
    "Python Cloud AI Docker Kubernetes Rust\n",
    "\"\"\"\n",
    "\n",
    "#1. Covert the unstructured to structured data using RDD\n",
    "sc = spark.sparkContext\n",
    "unstruct_rdd1 = sc.textFile(\"file:///home/hduser/samplecourse.log\")\n",
    "struct_rdd2 = unstruct_rdd1.flatMap(lambda row:row.split(\" \")) #['Python', 'Cloud', 'AI', 'Docker', 'Kubernetes']\n",
    "struct_schema_rdd3 = struct_rdd2.map(lambda word:[word]) #[['Python'], ['Cloud'], ['AI'], ['Docker'], ['Kubernetes']]\n",
    "\n",
    "#2. Covert the RDD into DF for further analysis\n",
    "df1 = struct_schema_rdd3.toDF()\n",
    "df1.createOrReplaceTempView(\"course_view\")\n",
    "df_view = spark.sql(\"select distinct _1 as distinct_courses from course_view\")\n",
    "df_view.show(5)\n",
    "\n",
    "df_view = spark.sql(\"select _1 as cource, count(*) as no_of_people_interested from course_view group by _1\")\n",
    "df_view.show(5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23b1051f-e941-477e-9fde-65cc2b12881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example structured RDD records:\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1063.partitions.\n: java.net.ConnectException: Call From localhost.localdomain/127.0.0.1 to localhost:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy41.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy42.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)\n\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:115)\n\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:349)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:202)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2142)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:276)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat jdk.internal.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)\n\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1502)\n\t... 49 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Print some results to verify\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample structured RDD records:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstructured_log_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(row)\n",
      "File \u001b[0;32m~/PycharmProjects/bigdataProjects/venv/lib/python3.9/site-packages/pyspark/rdd.py:2822\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2781\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2782\u001b[0m \u001b[38;5;124;03mTake the first num elements of the RDD.\u001b[39;00m\n\u001b[1;32m   2783\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2819\u001b[0m \u001b[38;5;124;03m[91, 92, 93]\u001b[39;00m\n\u001b[1;32m   2820\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2821\u001b[0m items: List[T] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 2822\u001b[0m totalParts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2823\u001b[0m partsScanned \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   2825\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(items) \u001b[38;5;241m<\u001b[39m num \u001b[38;5;129;01mand\u001b[39;00m partsScanned \u001b[38;5;241m<\u001b[39m totalParts:\n\u001b[1;32m   2826\u001b[0m     \u001b[38;5;66;03m# The number of partitions to try in this iteration.\u001b[39;00m\n\u001b[1;32m   2827\u001b[0m     \u001b[38;5;66;03m# It is ok for this number to be greater than totalParts because\u001b[39;00m\n\u001b[1;32m   2828\u001b[0m     \u001b[38;5;66;03m# we actually cap it at totalParts in runJob.\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/bigdataProjects/venv/lib/python3.9/site-packages/pyspark/rdd.py:5453\u001b[0m, in \u001b[0;36mPipelinedRDD.getNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m-> 5453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m~/PycharmProjects/bigdataProjects/venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/PycharmProjects/bigdataProjects/venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/PycharmProjects/bigdataProjects/venv/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1063.partitions.\n: java.net.ConnectException: Call From localhost.localdomain/127.0.0.1 to localhost:54310 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat com.sun.proxy.$Proxy41.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy42.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)\n\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:115)\n\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:349)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:202)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2142)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:276)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:210)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat jdk.internal.reflect.GeneratedMethodAccessor91.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.net.ConnectException: Connection refused\n\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)\n\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1502)\n\t... 49 more\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'sc' is your SparkContext\n",
    "\n",
    "# Your syslog data\n",
    "data = \"\"\"\n",
    "Aug 6 10:05:01 my-server-name systemd[1]: Started User Manager for UID 1000.\n",
    "Aug 6 10:05:02 my-server-name kernel: [ 10.123456] Bluetooth: BNEP (Ethernet Emulation) ver 1.3\n",
    "Aug 6 10:05:03 my-server-name sshd[12345]: Accepted password for user1 from 192.168.1.5 port 55678 ssh2\n",
    "Aug 6 10:05:04 my-server-name CRON[23456]: (root) CMD (command -v debian-sa1 > /dev/null && debian-sa1 1 1)\n",
    "Aug 6 10:05:05 my-server-name postfix/smtpd[34567]: connect from mail-server.example.com[203.0.113.10]\n",
    "Aug 6 10:05:06 my-server-name anacron[45678]: Job `cron.daily' terminated\n",
    "Aug 6 10:05:07 my-server-name systemd[1]: user@1000.service: Succeeded.\n",
    "Aug 6 10:05:08 my-server-name kernel: [ 20.987654] usb 1-1.2: new high-speed USB device number 3 using xhci_hcd\n",
    "\"\"\"\n",
    "\n",
    "# Write the sample data to a file\n",
    "with open(\"/home/hduser/syslog\", \"w\") as f:\n",
    "    f.write(data)\n",
    "\n",
    "# Read the file into an RDD\n",
    "raw_log_rdd = sc.textFile(\"/home/hduser/syslog\")\n",
    "\n",
    "import re\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def parse_log_line_rdd(line):\n",
    "    \"\"\"\n",
    "    Parses a single log line using a regular expression.\n",
    "    Returns a Row object with parsed fields.\n",
    "    \"\"\"\n",
    "    regex = r\"(\\w{3}\\s+\\d+\\s+\\d{2}:\\d{2}:\\d{2})\\s+([\\w.-]+)\\s+([\\w/]+)(?:\\[(\\d+)\\])?:\\s+(.*)\"\n",
    "    match = re.match(regex, line)\n",
    "    \n",
    "    if match:\n",
    "        # Extract fields from regex groups\n",
    "        timestamp_str, hostname, process_name, pid_str, message = match.groups()\n",
    "        pid = int(pid_str) if pid_str else None\n",
    "        \n",
    "        return Row(\n",
    "            timestamp=timestamp_str,\n",
    "            hostname=hostname,\n",
    "            process=process_name,\n",
    "            pid=pid,\n",
    "            message=message.strip()\n",
    "        )\n",
    "    else:\n",
    "        # For unparseable lines, return a Row with an error message\n",
    "        return Row(timestamp=None, hostname=None, process=None, pid=None, message=line, parse_error=True)\n",
    "\n",
    "# Apply the parsing function to the RDD\n",
    "structured_log_rdd = raw_log_rdd.map(parse_log_line_rdd)\n",
    "\n",
    "# Print some results to verify\n",
    "print(\"Example structured RDD records:\")\n",
    "for row in structured_log_rdd.take(3):\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f8fba-f5eb-422b-b9da-9a3aec6e3737",
   "metadata": {},
   "source": [
    "## **2. Reading a CSV data and write into MySql(RDBMS) Database using JDBC Option**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c7fe856-1806-44e6-95d4-30b398a79831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[INFO] df1.count() = 9999\n"
     ]
    }
   ],
   "source": [
    "###### Reading CSV data and write into DataFrame #######\n",
    "\n",
    "# Sample Customer Info Data\n",
    "\"\"\"\n",
    "cd /home/hduser/custinfo.csv\n",
    "\n",
    "4000001,Kristina,Chung,55,Pilot\n",
    "4000002,Paige,Chen,77,Teacher\n",
    "4000003,Sherri,Melton,34,Firefighter\n",
    "4000004,Gretchen,Hill,66,Computer hardware engineer\n",
    "4000005,Karen,Puckett,74,Lawyer\n",
    "\"\"\"\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Schema Definition\n",
    "custinfo_schema = StructType([StructField('custid', IntegerType(), True), StructField('first_name', StringType(), True), StructField('last_name', StringType(), True), StructField('age', IntegerType(), True), StructField('profession', StringType(), True)])\n",
    "\n",
    "# CSV Data Read and storing it in DataFrame\n",
    "df1 = spark.read.csv(path=\"file:///home/hduser/custinfo.csv\",header=False,sep=\",\",inferSchema=False,schema=custinfo_schema)\n",
    "df1.show(truncate=False,n=5)\n",
    "print(f\"[INFO] df1.count() = {df1.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0aba4a-95f7-45ca-bfea-6324679cd6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CSV file data write into MySQL DB is successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "###### Write the data into MySql DB ######\n",
    "\n",
    "# JDBC Options\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb?createDatabaseIfNotExist=true'\n",
    "dbproperties={'user':'root','password':'Root123$','driver':'com.mysql.cj.jdbc.Driver'}\n",
    "\n",
    "# Write into DB\n",
    "df1.write.jdbc(url=url1,properties=dbproperties,table=\"custinfo\",mode=\"overwrite\")\n",
    "print(\"[INFO] CSV file data write into MySQL DB is successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac17921-76c7-44e6-8fe1-138577b7f6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Simple way to read the data from MySql/RDBMS DB using JDBC ######\n",
    "\n",
    "# JDBC Options\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb'\n",
    "dbproperties={'user':'root','password':'Root123$','driver':'com.mysql.cj.jdbc.Driver'}\n",
    "\n",
    "# Read the data from RDBMS using query instead of direct table\n",
    "table_query = \"(select * from stocksdb.custinfo) as tablename\"\n",
    "df2_db = spark.read.jdbc(url=url1,properties=dbproperties,table=table_query)\n",
    "df2_db.cache()\n",
    "df2_db.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "210d08d4-984d-4630-bbb3-6b70ac7637f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Optimized way to read the data from any RDBMS DB using JDBC ######\n",
    "\n",
    "#Question: How to improve performance for JDBC?\n",
    "#partition, fetchsize, caching, pushdown optimization etc.,\n",
    "#partitionColumn:, numberOfPartitions:, upperBound:, lowerBound, predicates, fetchsize..\n",
    "\n",
    "# JDBC Options for performance optimization\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb'\n",
    "dbproperties = {\n",
    "    'user': 'root',\n",
    "    'password': 'Root123$',\n",
    "    'driver': 'com.mysql.cj.jdbc.Driver',\n",
    "    # Performance optimization options (values as strings):\n",
    "    'partitionColumn': 'custid',\n",
    "    'lowerBound': '4000001',  # Column used to divide data into sections for parallel processing.\n",
    "    'upperBound': '4000100',  # Minimum value for the partition column to start reading data.\n",
    "    'numPartitions': '3',     # Maximum value for the partition column to start reading data.\n",
    "    'pushDownPredicate': 'true',  # Sends filters (WHERE clauses) to the database for early processing.\n",
    "    'pushDownAggregate': 'true',  # Sends aggregations (SUM, COUNT) to the database for early processing.\n",
    "    'queryTimeout': '120',    # Maximum time (in seconds) a database query can run before timing out.\n",
    "    'fetchSize': '10',        # Number of rows retrieved from the database in each batch.\n",
    "    'isolationLevel': 'READ_COMMITTED' # Ensures only committed data is visible during a transaction.\n",
    "}\n",
    "\n",
    "# Read the data from RDBMS using query instead of direct table\n",
    "table_query = \"(select * from stocksdb.custinfo) as tablename\"\n",
    "df2_db = spark.read.jdbc(url=url1,properties=dbproperties,table=table_query)\n",
    "df2_db.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c3ddf-7f63-496e-be76-c7515bee8d91",
   "metadata": {},
   "source": [
    "## **3. Schema Evoluation/Growing handling using columner file formats ORC/Parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8063682e-e89b-4789-9603-f9b093f76c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORC/PARQUET Other Properties\n",
    "\n",
    "#Source is sending data on a daily basis, once in a while the schema of the data is evolving/growing\n",
    "  #Example (Day1): exch~stock~price\n",
    "  #Example (Day2): exch~stock~price~buyer\n",
    "  #Example (Day3): stock~price~seller\n",
    "\n",
    "#**mergeSchema: Orc/Parquet read all the datafiles headers and merge them into one header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d797ccc0-0dda-499b-9e55-a70c698013df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Day1 : Source CSV data\n",
      "+----+-----+-----+\n",
      "|exch|stock|price|\n",
      "+----+-----+-----+\n",
      "|NYSE|  CLI| 36.3|\n",
      "|NYSE|  ABC| 36.3|\n",
      "+----+-----+-----+\n",
      "\n",
      "[INFO] Day1 : ORC data read\n",
      "+----+-----+-----+\n",
      "|exch|stock|price|\n",
      "+----+-----+-----+\n",
      "|NYSE|  CLI| 36.3|\n",
      "|NYSE|  ABC| 36.3|\n",
      "+----+-----+-----+\n",
      "\n",
      "[INFO] Day2 : Source CSV data\n",
      "+----+-----+-----+------+\n",
      "|exch|stock|price| buyer|\n",
      "+----+-----+-----+------+\n",
      "|NYSE|  CLI| 37.3|  Alan|\n",
      "|NYSE|  ABC| 37.3|Harpar|\n",
      "+----+-----+-----+------+\n",
      "\n",
      "[INFO] Day2 : ORC data read with evolved schema\n",
      "+----+-----+-----+------+\n",
      "|exch|stock|price| buyer|\n",
      "+----+-----+-----+------+\n",
      "|NYSE|  CLI| 37.3|  Alan|\n",
      "|NYSE|  ABC| 37.3|Harpar|\n",
      "|NYSE|  CLI| 36.3|  NULL|\n",
      "|NYSE|  ABC| 36.3|  NULL|\n",
      "+----+-----+-----+------+\n",
      "\n",
      "[INFO] Day3 : Source CSV data\n",
      "+-----+-----+------+\n",
      "|stock|price|seller|\n",
      "+-----+-----+------+\n",
      "|  CLI| 37.3|  Jack|\n",
      "|  ABC| 37.3|  Ross|\n",
      "+-----+-----+------+\n",
      "\n",
      "[INFO] Day3 : ORC data read evolved schema\n",
      "+----+-----+-----+------+------+\n",
      "|exch|stock|price| buyer|seller|\n",
      "+----+-----+-----+------+------+\n",
      "|NYSE|  CLI| 37.3|  Alan|  NULL|\n",
      "|NYSE|  ABC| 37.3|Harpar|  NULL|\n",
      "|NYSE|  CLI| 36.3|  NULL|  NULL|\n",
      "|NYSE|  ABC| 36.3|  NULL|  NULL|\n",
      "|NULL|  CLI| 37.3|  NULL|  Jack|\n",
      "|NULL|  ABC| 37.3|  NULL|  Ross|\n",
      "+----+-----+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "day1 = \"\"\"\n",
    "exch~stock~price\n",
    "NYSE~CLI~36.3\n",
    "NYSE~ABC~36.3\n",
    "\"\"\"\n",
    "\n",
    "day2 = \"\"\"\n",
    "exch~stock~price~buyer\n",
    "NYSE~CLI~37.3~Alan\n",
    "NYSE~ABC~37.3~Harpar\n",
    "\"\"\"\n",
    "\n",
    "day3 = \"\"\"\n",
    "stock~price~seller\n",
    "CLI~37.3~Jack\n",
    "ABC~37.3~Ross\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "/home/hduser/stockdata_csv/\n",
    "├── part-00000-01f262bb-27a7-465d-95ca-4fdb6e1986aa-c000.csv\n",
    "└── _SUCCESS\n",
    "\"\"\"\n",
    "\n",
    "# Write the same data into CSV + Read the CSV + Write into ORC format (Append) + Read the ORC data (MergeSchema=True) \n",
    "\n",
    "# Day 1: exch~stock~price\n",
    "lines_day1 = day1.strip().split('\\n')\n",
    "header_day1 = lines_day1[0].split('~')\n",
    "data_rows_day1 = [line.split('~') for line in lines_day1[1:]]\n",
    "df1 = spark.createDataFrame(data_rows_day1, header_day1)\n",
    "df1.coalesce(1).write.csv(path=\"file:///home/hduser/stockdata_csv/\",mode=\"overwrite\",sep=\"~\",header=True)\n",
    "\n",
    "df_csv = spark.read.csv(path=\"file:///home/hduser/stockdata_csv/\",pathGlobFilter=\"part-*.csv\",sep=\"~\",header=True)\n",
    "print(\"[INFO] Day1 : Source CSV data\")\n",
    "df_csv.show()\n",
    "df_csv.coalesce(1).write.orc(path=\"file:///home/hduser/stockdata_orc/\",mode=\"overwrite\") # Overwrite for the first time\n",
    "df_orc = spark.read.orc(path=\"file:///home/hduser/stockdata_orc/\",mergeSchema=True) # Schema Evoluation\n",
    "print(\"[INFO] Day1 : ORC data read\")\n",
    "df_orc.show()                         \n",
    "\n",
    "# Day 2: exch~stock~price~buyer\n",
    "lines_day2 = day2.strip().split('\\n')\n",
    "header_day2 = lines_day2[0].split('~')\n",
    "data_rows_day2 = [line.split('~') for line in lines_day2[1:]]\n",
    "df2 = spark.createDataFrame(data_rows_day2, header_day2)\n",
    "df2.coalesce(1).write.csv(path=\"file:///home/hduser/stockdata_csv/\",mode=\"overwrite\",sep=\"~\",header=True)\n",
    "\n",
    "df_csv = spark.read.csv(path=\"file:///home/hduser/stockdata_csv/\",pathGlobFilter=\"part-*.csv\",sep=\"~\",header=True)\n",
    "print(\"[INFO] Day2 : Source CSV data\")\n",
    "df_csv.show()\n",
    "df_csv.coalesce(1).write.orc(path=\"file:///home/hduser/stockdata_orc/\",mode=\"append\") # Append for the Schema Evoluation\n",
    "df_orc = spark.read.orc(path=\"file:///home/hduser/stockdata_orc/\",mergeSchema=True) # Schema Evoluation\n",
    "print(\"[INFO] Day2 : ORC data read with evolved schema\")\n",
    "df_orc.show()    \n",
    "\n",
    "# Day 3: stock~price~seller\n",
    "lines_day3 = day3.strip().split('\\n')\n",
    "header_day3 = lines_day3[0].split('~')\n",
    "data_rows_day3 = [line.split('~') for line in lines_day3[1:]]\n",
    "df3 = spark.createDataFrame(data_rows_day3, header_day3)\n",
    "print(\"[INFO] Day3 : Source CSV data\")\n",
    "df3.show()\n",
    "df3.coalesce(1).write.csv(path=\"file:///home/hduser/stockdata_csv/\",mode=\"overwrite\",sep=\"~\",header=True)\n",
    "\n",
    "df_csv = spark.read.csv(path=\"file:///home/hduser/stockdata_csv/\",pathGlobFilter=\"part-*.csv\",sep=\"~\",header=True)\n",
    "df_csv.coalesce(1).write.orc(path=\"file:///home/hduser/stockdata_orc/\",mode=\"append\") # Append for the Schema Evoluation\n",
    "df_orc = spark.read.orc(path=\"file:///home/hduser/stockdata_orc/\",mergeSchema=True) # Schema Evoluation\n",
    "print(\"[INFO] Day3 : ORC data read evolved schema\")\n",
    "df_orc.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704e59a-cbf4-4d70-bea9-584a877b04ec",
   "metadata": {},
   "source": [
    "## **4. Reading a JSON data with various options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efbfbcc3-d829-430c-b989-5292b793a4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: decimal(10,3) (nullable = true)\n",
      " |-- isActive: boolean (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- street: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |-- date_joined: date (nullable = true)\n",
      " |-- timestamp_event: timestamp (nullable = true)\n",
      " |-- corrupted_record: string (nullable = true)\n",
      "\n",
      "+----+-------+----+---------+--------+-------------------------+------+-------------------------+-----------+-----------------------+----------------+\n",
      "|id  |name   |age |salary   |isActive|comments                 |tags  |address                  |date_joined|timestamp_event        |corrupted_record|\n",
      "+----+-------+----+---------+--------+-------------------------+------+-------------------------+-----------+-----------------------+----------------+\n",
      "|1   |Alice  |30  |50000.500|true    |This is a comment.       |[A, B]|{123 Main St, Anytown}   |NULL       |NULL                   |NULL            |\n",
      "|2   |Bob    |25  |45000.750|false   |Another comment.         |[C]   |{456 Oak Ave, Otherville}|NULL       |NULL                   |NULL            |\n",
      "|3   |Charlie|NULL|NULL     |true    |Invalid JSON             |[D, E]|NULL                     |NULL       |NULL                   |NULL            |\n",
      "|4   |David  |40  |60000.000|true    |NULL                     |NULL  |NULL                     |2023-01-15 |2023-01-15 10:30:00.123|NULL            |\n",
      "|5   |Eve    |35  |55555.555|true    |This has 'single quotes'.|NULL  |NULL                     |NULL       |NULL                   |NULL            |\n",
      "|6   |Frank  |28  |12345.678|true    |Escaped chars: \\n\\t\\r    |[F]   |NULL                     |NULL       |NULL                   |NULL            |\n",
      "|NULL|NULL   |NULL|NULL     |NULL    |NULL                     |NULL  |NULL                     |NULL       |NULL                   |NULL            |\n",
      "+----+-------+----+---------+--------+-------------------------+------+-------------------------+-----------+-----------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DecimalType,BooleanType,ArrayType,DateType,TimestampType\n",
    "\n",
    "# Data\n",
    "samplejson = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"name\": \"Alice\",\n",
    "    \"age\": 30,\n",
    "    \"salary\": 50000.50,\n",
    "    \"isActive\": true,\n",
    "    \"comments\": \"This is a comment.\",\n",
    "    \"tags\": [\"A\", \"B\"],\n",
    "    \"address\": {\n",
    "      \"street\": \"123 Main St\",\n",
    "      \"city\": \"Anytown\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2,\n",
    "    \"name\": \"Bob\",\n",
    "    \"age\": 25,\n",
    "    \"salary\": 45000.75,\n",
    "    \"isActive\": false,\n",
    "    \"comments\": \"Another comment.\",\n",
    "    \"tags\": [\"C\"],\n",
    "    \"address\": {\n",
    "      \"street\": \"456 Oak Ave\",\n",
    "      \"city\": \"Otherville\"\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"id\": 3,\n",
    "    \"name\": \"Charlie\",\n",
    "    \"age\": null,\n",
    "    \"salary\": null,\n",
    "    \"isActive\": true,\n",
    "    \"comments\": \"Invalid JSON\",\n",
    "    \"tags\": [\"D\", \"E\"]\n",
    "  },\n",
    "  {\n",
    "    \"id\": 4,\n",
    "    \"name\": \"David\",\n",
    "    \"age\": 40,\n",
    "    \"salary\": 60000.00,\n",
    "    \"isActive\": true,\n",
    "    \"date_joined\": \"2023-01-15\",\n",
    "    \"timestamp_event\": \"2023-01-15 10:30:00.123\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 5,\n",
    "    \"name\": \"Eve\",\n",
    "    \"age\": 35,\n",
    "    \"salary\": 55555.555,\n",
    "    \"isActive\": true,\n",
    "    \"comments\": \"This has 'single quotes'.\",\n",
    "    \"field with space\": \"value\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 6,\n",
    "    \"name\": \"Frank\",\n",
    "    \"age\": 28,\n",
    "    \"salary\": 12345.678,\n",
    "    \"isActive\": true,\n",
    "    \"comments\": \"Escaped chars: \\\\n\\\\t\\\\r\",\n",
    "    \"tags\": [\"F\"],\n",
    "    \"decimal_val\": 12345.678\n",
    "  },\n",
    "  {\n",
    "    \"invalid\":\"invalid\"\n",
    "  }  \n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# JSON file creation\n",
    "import os\n",
    "file_path = \"/home/hduser/employe_json/sample.json\"\n",
    "directory = os.path.dirname(file_path)\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "with open(\"/home/hduser/employe_json/sample.json\", \"w\") as f:\n",
    "    f.write(samplejson)\n",
    "\n",
    "# Define a custom schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", DecimalType(10, 3), True),\n",
    "    StructField(\"isActive\", BooleanType(), True),\n",
    "    StructField(\"comments\", StringType(), True),\n",
    "    StructField(\"tags\", ArrayType(StringType()), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"street\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True)\n",
    "    ])),\n",
    "    StructField(\"date_joined\", DateType(), True),\n",
    "    StructField(\"timestamp_event\", TimestampType(), True),\n",
    "    StructField(\"corrupted_record\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Read JSON with multiple options and inline comments\n",
    "df = spark.read.json(\n",
    "    path=\"file:///home/hduser/employe_json/\",  # Specifies the location of the JSON file(s) to read.\n",
    "    schema=custom_schema,  # Defines a custom schema to avoid automatic inference.\n",
    "    primitivesAsString=False,  # Treats all primitive values (int, float, bool) as strings if True.\n",
    "    prefersDecimal=True,  # Infers floating-point numbers as DecimalType instead of DoubleType.\n",
    "    allowComments=True,  # Allows Java/C++ style comments (//, /* */) in JSON.\n",
    "    allowUnquotedFieldNames=True,  # Accepts JSON keys without double quotes.\n",
    "    allowSingleQuotes=True,  # Accepts single quotes for string values.\n",
    "    allowBackslashEscapingAnyCharacter=True,  # Allows any character to be escaped with a backslash.\n",
    "    mode=\"PERMISSIVE\",  # Defines how to handle corrupt records (PERMISSIVE, DROPMALFORMED, FAILFAST).\n",
    "    columnNameOfCorruptRecord=\"corrupted_record\",  # Stores malformed JSON strings in a specified column.\n",
    "    dateFormat=\"yyyy-MM-dd\",  # Specifies the format for parsing date strings.\n",
    "    timestampFormat=\"yyyy-MM-dd HH:mm:ss.SSS\",  # Specifies the format for parsing timestamp strings.\n",
    "    multiLine=True,  # Treats the entire file as a single JSON object (for pretty-printed or array JSON).\n",
    "    allowUnquotedControlChars=True,  # Allows control characters (e.g., \\n, \\t) to appear unquoted.\n",
    "    lineSep=\"\\n\",  # Defines a custom line separator between JSON records. If multiline=True then it is not required.\n",
    "    samplingRatio=1.0,  # Sets the fraction of data used for schema inference.\n",
    "    encoding=\"UTF-8\",  # Specifies the character encoding (e.g., UTF-8, UTF-16).\n",
    "    locale=\"en-US\",  # Sets the locale for parsing locale-sensitive data like dates.\n",
    "    pathGlobFilter=\"*.json\",  # Filters files using glob patterns (e.g., *.json).\n",
    "    recursiveFileLookup=True  # Enables recursive search in subdirectories.\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645d216-3f64-40c3-838e-d9a9c79372db",
   "metadata": {},
   "source": [
    "## **5. Reading a CSV data with various options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "315837f5-9161-4ae0-866a-453314f0feaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "|symbol                           |exchange|date      |timestamp          |price   |volume |corrupted_data                                     |\n",
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "|AAPL                             |NYSE    |2023-08-01|2023-08-01 09:30:00|195.25  |1200000|NULL                                               |\n",
      "|GOOGL                            |NYSE    |2023-08-01|2023-08-01 09:30:00|2735.55 |850000 |NULL                                               |\n",
      "|MSFT                             |NYSE    |2023-08-01|2023-08-01 09:30:00|NaN     |950000 |NULL                                               |\n",
      "|TSLA                             |NYSE    |2023-08-01|2023-08-01 09:30:00|Infinity|1100000|NULL                                               |\n",
      "|AMZN                             |NYSE    |NULL      |2023-08-01 09:30:00|134.25  |NULL   |NULL                                               |\n",
      "|MSFK                             |NYSE    |2023-08-01|NULL               |100.01  |950000 |MSFK,NYSE,2023-08-01,2023-08-01 09:30,100.01,950000|\n",
      "|INVALID_ROW_WITHOUT_PROPER_FIELDS|NULL    |NULL      |NULL               |NULL    |NULL   |INVALID_ROW_WITHOUT_PROPER_FIELDS                  |\n",
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "\n",
      "[INFO] Corruputed Rows\n",
      "+---------------------------------+--------+----------+---------+------+------+---------------------------------------------------+\n",
      "|symbol                           |exchange|date      |timestamp|price |volume|corrupted_data                                     |\n",
      "+---------------------------------+--------+----------+---------+------+------+---------------------------------------------------+\n",
      "|MSFK                             |NYSE    |2023-08-01|NULL     |100.01|950000|MSFK,NYSE,2023-08-01,2023-08-01 09:30,100.01,950000|\n",
      "|INVALID_ROW_WITHOUT_PROPER_FIELDS|NULL    |NULL      |NULL     |NULL  |NULL  |INVALID_ROW_WITHOUT_PROPER_FIELDS                  |\n",
      "+---------------------------------+--------+----------+---------+------+------+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Data\n",
    "data = \"\"\"\n",
    "symbol,exchange,date,timestamp,price,volume\n",
    "AAPL,NYSE,2023-08-01,2023-08-01 09:30:00,195.25,1200000\n",
    "GOOGL,NYSE,2023-08-01,2023-08-01 09:30:00,2735.55,850000\n",
    "MSFT,NYSE,2023-08-01,2023-08-01 09:30:00,-1,950000\n",
    "TSLA,NYSE,2023-08-01,2023-08-01 09:30:00,Inf,1100000\n",
    "AMZN,NYSE,na,2023-08-01 09:30:00,134.25,na\n",
    "MSFK,NYSE,2023-08-01,2023-08-01 09:30,100.01,950000\n",
    "INVALID_ROW_WITHOUT_PROPER_FIELDS       \n",
    "\"\"\"\n",
    "\n",
    "# CSV file creation\n",
    "file_path = \"/home/hduser/employe_csv/sample.csv\"\n",
    "directory = os.path.dirname(file_path)\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(data)\n",
    "\n",
    "# Define custom schema\n",
    "customschema = StructType([\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"exchange\", StringType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"volume\", IntegerType(), True),\n",
    "    StructField(\"corrupted_data\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read CSV with various options\n",
    "df1 = spark.read.csv(\n",
    "    path=\"file:///home/hduser/employe_csv/sample.csv\",\n",
    "    sep=',',  # Column separator used in the CSV file\n",
    "    header=True,  # First line of the file contains column headers\n",
    "    schema=customschema,  # Custom schema to define data types and structure\n",
    "    columnNameOfCorruptRecord='corrupted_data',  # Stores malformed rows in this column\n",
    "    encoding='UTF-8',  # Character encoding used to read the file\n",
    "    quote=\"'\",  # Defines single quote as the string quoting character\n",
    "    comment='-',  # Lines starting with '-' are treated as comments and ignored\n",
    "    ignoreTrailingWhiteSpace=True,  # Trims trailing whitespace from fields\n",
    "    ignoreLeadingWhiteSpace=True,  # Trims leading whitespace from fields\n",
    "    nullValue='na',  # Treats 'na' as a null value\n",
    "    nanValue='-1',  # Treats '-1' as NaN (Not a Number)\n",
    "    positiveInf='Inf',  # Treats 'Inf' as positive infinity\n",
    "    dateFormat='yyyy-MM-dd',  # Format used to parse date fields\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',  # Format used to parse timestamp fields\n",
    "    maxColumns=40  # Maximum number of columns allowed in the file\n",
    ")\n",
    "\n",
    "# Show first 10 rows\n",
    "df1.show(10, False)\n",
    "\n",
    "print(\"[INFO] Corruputed Rows\")\n",
    "# Cache and filter corrupted rows\n",
    "df2 = df1.cache().where(\"corrupted_data is not null\")\n",
    "df2.show(10, False)  # Display malformed rows for RCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70210465-05af-4071-b1d4-83e0296c48b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **6. ORC & Parquet file format for Performance Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fad88c1-78cc-4719-b956-7984dbdcec60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Scrubbed Data\n",
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "|symbol                           |exchange|date      |timestamp          |price   |volume |corrupted_data                                     |\n",
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "|AAPL                             |NYSE    |2023-08-01|2023-08-01 09:30:00|195.25  |1200000|NULL                                               |\n",
      "|GOOGL                            |NYSE    |2023-08-01|2023-08-01 09:30:00|2735.55 |850000 |NULL                                               |\n",
      "|MSFT                             |NYSE    |2023-08-01|2023-08-01 09:30:00|NaN     |950000 |NULL                                               |\n",
      "|TSLA                             |NYSE    |2023-08-01|2023-08-01 09:30:00|Infinity|1100000|NULL                                               |\n",
      "|AMZN                             |NYSE    |NULL      |2023-08-01 09:30:00|134.25  |NULL   |NULL                                               |\n",
      "|MSFK                             |NYSE    |2023-08-01|NULL               |100.01  |950000 |MSFK,NYSE,2023-08-01,2023-08-01 09:30,100.01,950000|\n",
      "|INVALID_ROW_WITHOUT_PROPER_FIELDS|NULL    |NULL      |NULL               |NULL    |NULL   |INVALID_ROW_WITHOUT_PROPER_FIELDS                  |\n",
      "+---------------------------------+--------+----------+-------------------+--------+-------+---------------------------------------------------+\n",
      "\n",
      "[INFO] Cureated Data\n",
      "+------+--------+----------+-------------------+--------+-------+--------------+\n",
      "|symbol|exchange|date      |timestamp          |price   |volume |corrupted_data|\n",
      "+------+--------+----------+-------------------+--------+-------+--------------+\n",
      "|AAPL  |NYSE    |2023-08-01|2023-08-01 09:30:00|195.25  |1200000|NULL          |\n",
      "|GOOGL |NYSE    |2023-08-01|2023-08-01 09:30:00|2735.55 |850000 |NULL          |\n",
      "|MSFT  |NYSE    |2023-08-01|2023-08-01 09:30:00|NaN     |950000 |NULL          |\n",
      "|TSLA  |NYSE    |2023-08-01|2023-08-01 09:30:00|Infinity|1100000|NULL          |\n",
      "|AMZN  |NYSE    |NULL      |2023-08-01 09:30:00|134.25  |NULL   |NULL          |\n",
      "+------+--------+----------+-------------------+--------+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 21:45:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stock_symbol, exchange, date, timestamp, price, volume\n",
      " Schema: symbol, exchange, date, timestamp, price, volume\n",
      "Expected: symbol but found: stock_symbol\n",
      "CSV file: file:///tmp/nyse_header_options2.csv\n",
      "25/08/05 21:45:04 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stock_symbol, exchange, date, timestamp, price, volume\n",
      " Schema: symbol, exchange, date, timestamp, price, volume\n",
      "Expected: symbol but found: stock_symbol\n",
      "CSV file: file:///tmp/nyse_header_options2.csv\n"
     ]
    }
   ],
   "source": [
    "# Data SCHEMA MIGRATION from csv (Struct) to ORC/Parquet (serialized-binary) and load into our DATALAKE\n",
    "\n",
    "# Data\n",
    "data =\"\"\" \n",
    "stock_symbol,exchange,date,timestamp,price,volume\n",
    "AAPL,NYSE,2023-08-01,2023-08-01 09:30:00,195.25,1200000\n",
    "GOOGL,NYSE,2023-08-01,2023-08-01 09:30:00,2735.55,850000\n",
    "MSFT,NYSE,2023-08-01,2023-08-01 09:30:00,-1,950000\n",
    "TSLA,NYSE,2023-08-01,2023-08-01 09:30:00,Inf,1100000\n",
    "AMZN,NYSE,na,2023-08-01 09:30:00,134.25,na\n",
    "MSFK,NYSE,2023-08-01,2023-08-01 09:30,100.01,950000\n",
    "INVALID_ROW_WITHOUT_PROPER_FIELDS \n",
    "\"\"\"\n",
    " \n",
    "with open(\"/tmp/nyse_header_options2.csv\", \"w\") as f:\n",
    "    f.write(data)\n",
    "  \n",
    "# Define custom schema\n",
    "customschema = StructType([\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"exchange\", StringType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"volume\", IntegerType(), True),\n",
    "    StructField(\"corrupted_data\", StringType(), True)\n",
    "])\n",
    " \n",
    "# Read CSV with various options\n",
    "df1 = spark.read.csv(\n",
    "    'file:///tmp/nyse_header_options2.csv',\n",
    "    sep=',',  # Column separator used in the CSV file\n",
    "    header=True,  # First line of the file contains column headers\n",
    "    schema=customschema,  # Custom schema to define data types and structure\n",
    "    columnNameOfCorruptRecord='corrupted_data',  # Stores malformed rows in this column\n",
    "    encoding='UTF-8',  # Character encoding used to read the file\n",
    "    quote=\"'\",  # Defines single quote as the string quoting character\n",
    "    comment='-',  # Lines starting with '-' are treated as comments and ignored\n",
    "    ignoreTrailingWhiteSpace=True,  # Trims trailing whitespace from fields\n",
    "    ignoreLeadingWhiteSpace=True,  # Trims leading whitespace from fields\n",
    "    nullValue='na',  # Treats 'na' as a null value\n",
    "    nanValue='-1',  # Treats '-1' as NaN (Not a Number)\n",
    "    positiveInf='Inf',  # Treats 'Inf' as positive infinity\n",
    "    dateFormat='yyyy-MM-dd',  # Format used to parse date fields\n",
    "    timestampFormat='yyyy-MM-dd HH:mm:ss',  # Format used to parse timestamp fields\n",
    "    maxColumns=40  # Maximum number of columns allowed in the file\n",
    " \n",
    ")\n",
    " \n",
    "# Show first 10 rows\n",
    "print(\"[INFO] Scrubbed Data\")\n",
    "df1.show(10, False)\n",
    " \n",
    "# Cache and filter corrupted rows\n",
    "df2 = df1.cache().where(\"corrupted_data is null\")\n",
    "print(\"[INFO] Cureated Data\")\n",
    "df2.show(10, False)  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a0dc04d-e251-4bc2-8219-629152ccbf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading data from ORC\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|symbol|exchange|timestamp          |price   |volume |corrupted_data|date      |\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|GOOGL |NYSE    |2023-08-01 09:30:00|2735.55 |850000 |NULL          |2023-08-01|\n",
      "|MSFT  |NYSE    |2023-08-01 09:30:00|NaN     |950000 |NULL          |2023-08-01|\n",
      "|TSLA  |NYSE    |2023-08-01 09:30:00|Infinity|1100000|NULL          |2023-08-01|\n",
      "|AAPL  |NYSE    |2023-08-01 09:30:00|195.25  |1200000|NULL          |2023-08-01|\n",
      "|AMZN  |NYSE    |2023-08-01 09:30:00|134.25  |NULL   |NULL          |NULL      |\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "\n",
      "[INFO] Reading data from ORC using SQL\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|symbol|exchange|timestamp          |price   |volume |corrupted_data|date      |\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|GOOGL |NYSE    |2023-08-01 09:30:00|2735.55 |850000 |NULL          |2023-08-01|\n",
      "|MSFT  |NYSE    |2023-08-01 09:30:00|NaN     |950000 |NULL          |2023-08-01|\n",
      "|TSLA  |NYSE    |2023-08-01 09:30:00|Infinity|1100000|NULL          |2023-08-01|\n",
      "|AAPL  |NYSE    |2023-08-01 09:30:00|195.25  |1200000|NULL          |2023-08-01|\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data SCHEMA MIGRATION from csv (Struct) to ORC (serialized-binary) and load into our DATALAKE\n",
    "# Write the clean data in ORC format for internal teams data querying furpose\n",
    "df2.write.orc('file:///tmp/stocks_orc',mode='overwrite') #(Datalake) hdfs:///user/hduser/custorcout\n",
    "df2.write.orc('file:///tmp/stocks_orc_lzo',mode='ignore',compression='lzo')\n",
    "df2.write.orc('file:///tmp/stocks_orc_lzo_part',mode='overwrite',partitionBy='date')\n",
    "\n",
    "# Reading the data from ORC\n",
    "df_orc = spark.read.orc(\"file:///tmp/stocks_orc_lzo_part\")\n",
    "print(\"[INFO] Reading data from ORC\")\n",
    "df_orc.show(truncate=False)\n",
    " \n",
    "df_orc_sql = spark.sql(\"select * from orc.`file:///tmp/stocks_orc_lzo_part` where date is not null\")\n",
    "print(\"[INFO] Reading data from ORC using SQL\")\n",
    "df_orc_sql.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f15171f8-9fda-42cf-8362-24d63e1db2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reading data from Parquet\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|symbol|exchange|timestamp          |price   |volume |corrupted_data|date      |\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|GOOGL |NYSE    |2023-08-01 09:30:00|2735.55 |850000 |NULL          |2023-08-01|\n",
      "|MSFT  |NYSE    |2023-08-01 09:30:00|NaN     |950000 |NULL          |2023-08-01|\n",
      "|TSLA  |NYSE    |2023-08-01 09:30:00|Infinity|1100000|NULL          |2023-08-01|\n",
      "|AAPL  |NYSE    |2023-08-01 09:30:00|195.25  |1200000|NULL          |2023-08-01|\n",
      "|AMZN  |NYSE    |2023-08-01 09:30:00|134.25  |NULL   |NULL          |NULL      |\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "\n",
      "[INFO] Reading data from Parquet using SQL\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|symbol|exchange|timestamp          |price   |volume |corrupted_data|date      |\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "|GOOGL |NYSE    |2023-08-01 09:30:00|2735.55 |850000 |NULL          |2023-08-01|\n",
      "|MSFT  |NYSE    |2023-08-01 09:30:00|NaN     |950000 |NULL          |2023-08-01|\n",
      "|TSLA  |NYSE    |2023-08-01 09:30:00|Infinity|1100000|NULL          |2023-08-01|\n",
      "|AAPL  |NYSE    |2023-08-01 09:30:00|195.25  |1200000|NULL          |2023-08-01|\n",
      "+------+--------+-------------------+--------+-------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data SCHEMA MIGRATION from csv (Struct) to PARQUET (serialized-binary) and load into our DATALAKE\n",
    "# Write the clean data in ORC format for internal teams data querying furpose\n",
    "df2.write.parquet('file:///tmp/stocks_parquet',mode='overwrite') #(Datalake) hdfs:///user/hduser/custorcout\n",
    "df2.write.parquet('file:///tmp/stocks_parquet_lzo',mode='ignore',compression='snappy')\n",
    "df2.write.parquet('file:///tmp/stocks_parquet_part',mode='overwrite',partitionBy='date')\n",
    " \n",
    "# Reading the data from Parquet\n",
    "df_orc = spark.read.parquet(\"file:///tmp/stocks_parquet_part\")\n",
    "print(\"[INFO] Reading data from Parquet\")\n",
    "df_orc.show(truncate=False)\n",
    " \n",
    "df_orc_sql = spark.sql(\"select * from parquet.`file:///tmp/stocks_parquet_part` where date is not null\")\n",
    "print(\"[INFO] Reading data from Parquet using SQL\")\n",
    "df_orc_sql.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f06c24-b9e8-4e35-9157-5c1d6eaf2dc8",
   "metadata": {},
   "source": [
    "## **7. PySpark and Hive Integration : Data Ingestion and Table Creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b6b6c98-87c0-4d27-9455-1e9268612cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script demonstrates various methods for writing data from a PySpark\n",
    "# DataFrame into Hive tables, highlighting best practices, limitations,\n",
    "# and common use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89c26bec-f531-4609-bcd6-fdf978c483c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----------+---+--------------------+\n",
      "|    cid|   fname|     lname|age|          profession|\n",
      "+-------+--------+----------+---+--------------------+\n",
      "|4000001|Kristina|     Chung| 55|               Pilot|\n",
      "|4000002|   Paige|      Chen| 77|             Teacher|\n",
      "|4000003|  Sherri|    Melton| 34|         Firefighter|\n",
      "|4000004|Gretchen|      Hill| 66|Computer hardware...|\n",
      "|4000005|   Karen|   Puckett| 74|              Lawyer|\n",
      "|4000006| Patrick|      Song| 42|        Veterinarian|\n",
      "|4000007|   Elsie|  Hamilton| 43|               Pilot|\n",
      "|4000008|   Hazel|    Bender| 63|           Carpenter|\n",
      "|4000009| Malcolm|    Wagner| 39|              Artist|\n",
      "|4000010| Dolores|McLaughlin| 60|              Writer|\n",
      "|4000011| Francis|  McNamara| 47|           Therapist|\n",
      "|4000012|   Sandy|    Raynor| 26|              Writer|\n",
      "|4000013|  Marion|      Moon| 41|           Carpenter|\n",
      "|4000014|    Beth|   Woodard| 65|                NULL|\n",
      "|4000015|   Julia|     Desai| 49|            Musician|\n",
      "|4000016|  Jerome|   Wallace| 52|          Pharmacist|\n",
      "|4000017|    Neal|  Lawrence| 72|Computer support ...|\n",
      "|4000018|    Jean|   Griffin| 45|    Childcare worker|\n",
      "|4000019|Kristine| Dougherty| 63|   Financial analyst|\n",
      "|4000020| Crystal|    Powers| 67|Engineering techn...|\n",
      "+-------+--------+----------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ShortType\n",
    "\n",
    "# Placeholder for the data and schema\n",
    "cust_schema = StructType([\n",
    "    StructField('cid', IntegerType(), nullable=False),\n",
    "    StructField('fname', StringType()),\n",
    "    StructField('lname', StringType()),\n",
    "    StructField('age', ShortType()),\n",
    "    StructField('profession', StringType())\n",
    "])\n",
    "\n",
    "df1 = spark.read.csv(\n",
    "     'file:///home/hduser/custinfo.csv',\n",
    "     schema=cust_schema,\n",
    "     header=False,\n",
    "     sep=',',\n",
    "     mode='dropmalformed'\n",
    ")\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd840dc-69c8-4fb9-bd21-bf60df853dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Method 2: Using the `insertInto` function\n",
    "#\n",
    "# This method loads data into an existing table. It's less common for initial\n",
    "# table creation and data loading, as it does not create a new table schema.\n",
    "# ==============================================================================\n",
    "print(\"--- Method 2: Inserting data into an existing table using insertInto ---\")\n",
    "# This requires the 'default.customers' table to already exist.\n",
    "# The schema of the DataFrame must match the table schema.\n",
    "# df1.write.insertInto('wholesale.customers', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d09ba3-5b41-4c9b-8a09-ac3ce6e0d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Method 3: Storing as CSV (PySpark only)\n",
    "#\n",
    "# This method creates a table with data stored in CSV format. This table is\n",
    "# typically only accessible and readable via PySpark, not directly via HiveQL.\n",
    "# The SerDe (Serializer/Deserializer) for Spark-written CSVs is not\n",
    "# compatible with Hive's default TextFile SerDe.\n",
    "# ==============================================================================\n",
    "print(\"--- Method 3: Creating a CSV table (PySpark-only access) ---\")\n",
    "# Creates a managed table with data stored as CSV files.\n",
    "# Hive CLI will not be able to read this table correctly.\n",
    "df1.write.saveAsTable(\n",
    "    'default.customers_csv',\n",
    "    format='csv',\n",
    "    sep=',',\n",
    "    mode='overwrite'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fffd58-edbd-4751-a65d-3b1870699562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Method 4: Using Hive Literal Syntax\n",
    "#\n",
    "# This approach uses direct HiveQL statements to create a table that is\n",
    "# fully compatible with both Hive and PySpark. It is the proper way to\n",
    "# meet a requirement for a TextFile table with a specific delimiter.\n",
    "# ==============================================================================\n",
    "print(\"--- Method 4: Creating a TextFile table using HiveQL (interoperable) ---\")\n",
    "# Step 1: Create the table using HiveQL with the specified row format.\n",
    "spark.sql(\"\"\"\n",
    "   CREATE TABLE default.customers_text (\n",
    "        id INT,\n",
    "        fname STRING,\n",
    "        lname STRING,\n",
    "        age INT,\n",
    "        prof STRING\n",
    "    )\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    STORED AS TEXTFILE\n",
    "\"\"\")\n",
    "\n",
    "# Step 2: Load the data into the newly created table.\n",
    "# Note: 'local' means the file is on the driver's local filesystem.\n",
    "\n",
    "spark.sql(\n",
    "    \"LOAD DATA LOCAL INPATH 'file:///home/hduser/custinfo.csv' OVERWRITE INTO TABLE default.customers_text\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90269c1-a89a-4f79-9965-535d7e57ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Method 5: Marrying DataFrame to a Hive Table using a View and Insert Select\n",
    "#\n",
    "# This is a common pattern to load data from a DataFrame into an existing\n",
    "# Hive table, providing full interoperability.\n",
    "# ==============================================================================\n",
    "print(\"--- Method 5: Using Insert Select from a temporary view ---\")\n",
    "# Step 1: Create a temporary view from the DataFrame.\n",
    "df1.createOrReplaceTempView(\"view1\")\n",
    "\n",
    "# Step 2: Insert data from the view into an existing Hive table.\n",
    "# Assumes 'default.customers_text' exists and has a compatible schema.\n",
    "spark.sql(\"INSERT OVERWRITE TABLE default.customers_csv SELECT * FROM view1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ddb6aa-0422-44ff-a193-4ce50cf934b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Creating External Tables\n",
    "#\n",
    "# Demonstrates creating external tables, where the data is not managed by Hive,\n",
    "# using both non-partitioned and partitioned approaches.\n",
    "# ==============================================================================\n",
    "print(\"--- Creating a non-partitioned external table ---\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE default.customers_text_ext (\n",
    "        id INT,\n",
    "        fname STRING,\n",
    "        lname STRING,\n",
    "        age INT,\n",
    "        prof STRING\n",
    "    )\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    STORED AS TEXTFILE \n",
    "    LOCATION '/user/hduser/customer_ext_table'\n",
    "\"\"\")\n",
    "# Loading data into the external table.\n",
    "spark.sql(\n",
    "    \"LOAD DATA LOCAL INPATH 'file:///home/hduser/custinfo.csv' OVERWRITE INTO TABLE default.customers_text_ext\"\n",
    ")\n",
    "\n",
    "print(\"--- Creating a partitioned external table ---\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE default.customers_text_ext_part (\n",
    "        id INT,\n",
    "        fname STRING,\n",
    "        lname STRING,\n",
    "        age INT\n",
    "    )\n",
    "    PARTITIONED BY (prof STRING)\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    STORED AS TEXTFILE \n",
    "    LOCATION '/user/hduser/customer_ext_table_part'\n",
    "\"\"\")\n",
    "\n",
    "# Note: `LOAD DATA` does not support dynamic partitioning.\n",
    "# Instead, we use `INSERT OVERWRITE` with dynamic partitioning enabled.\n",
    "spark.sql(\"SET spark.sql.sources.partitionOverwriteMode=dynamic\") # Use Spark setting for partition overwrite mode\n",
    "spark.sql(\"SET hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "\n",
    "# Insert data dynamically into the partitioned table.\n",
    "spark.sql(\n",
    "    \"INSERT OVERWRITE TABLE default.customers_text_ext_part PARTITION(prof) SELECT id, fname, lname, age, prof FROM view1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ad275-4356-4798-a753-13198e9eae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Spark and Hive Bucketing Incompatibility\n",
    "#\n",
    "# This is a critical point about a known limitation. Spark and Hive use\n",
    "# different hashing algorithms for bucketing, making them incompatible.\n",
    "# ==============================================================================\n",
    "print(\"--- Demonstrating Spark-Hive Bucketing Incompatibility ---\")\n",
    "# Drop the table if it already exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS default.customers_text_ext_part_bucket\")\n",
    "\n",
    "# Create a bucketed table using HiveQL\n",
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE default.customers_text_ext_part_bucket (\n",
    "        id INT,\n",
    "        fname STRING,\n",
    "        lname STRING,\n",
    "        age INT,\n",
    "        prof STRING\n",
    "    )\n",
    "    CLUSTERED BY (id) INTO 10 BUCKETS\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "    STORED AS TEXTFILE \n",
    "    LOCATION '/user/hduser/customer_ext_table_part_bucket'\n",
    "\"\"\")\n",
    "\n",
    "# Attempting to load data from a DataFrame into this table will fail\n",
    "# because Spark's bucketing algorithm is not compatible with Hive's.\n",
    "# The following line will raise an AnalysisException.\n",
    "# spark.sql(\"INSERT OVERWRITE TABLE wholesale.customers_text_ext_part_bucket SELECT * FROM view1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922e125-8914-4457-a93b-c72477637d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Solution for High Performance: Combining Partitioning, Bucketing, and Columnar Format\n",
    "#\n",
    "# This code shows how to create a highly optimized table entirely within\n",
    "# PySpark, which is best for Spark-native queries.\n",
    "# Default format is Parquet to saving the data and Snappy codec for compression.\n",
    "# ==============================================================================\n",
    "print(\"--- Creating a highly performant table using PySpark native methods ---\")\n",
    "df1.write.bucketBy(10, 'cid').sortBy(\"cid\").\\\n",
    "    saveAsTable(\n",
    "        'default.customers_part_buck_parquet_snappy',\n",
    "        mode='overwrite',\n",
    "        partitionBy='profession'\n",
    "    )\n",
    "\n",
    "print(\"--- Conclusion on PySpark and Hive Integration ---\")\n",
    "print(\"PySpark and Hive have strong integration, but it's crucial to understand\")\n",
    "print(\"the differences in their internal implementations, especially concerning\")\n",
    "print(\"file formats and bucketing algorithms, to ensure interoperability and performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f769c9-0b34-4144-b601-f93e35c68cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
