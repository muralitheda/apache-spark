{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d55ae10-939c-4905-8f70-7f2a73350867",
   "metadata": {},
   "source": [
    "# **PySpark Ingestion + Egress + Dataloading Techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7fa8ec8-e8f6-4c26-a269-20bad66f4c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SparkSession Object Memory Reference: <pyspark.sql.session.SparkSession object at 0xffff7610b8e0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#MySql jdbc connector jar local path\n",
    "mysql_connector_jar_path = \"/home/hduser/install/mysql-connector-java.jar\"\n",
    "\n",
    "#Spark Session Creation\n",
    "spark =  SparkSession.builder\\\n",
    "    .appName(\"Spark-Ingress-Egress-Dataloading-Practice\")\\\n",
    "    .config(\"spark.jars\", mysql_connector_jar_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"[INFO] SparkSession Object Memory Reference: {spark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f8fba-f5eb-422b-b9da-9a3aec6e3737",
   "metadata": {},
   "source": [
    "## **1. Reading a CSV data and write into MySql Database using JDBC Option**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c7fe856-1806-44e6-95d4-30b398a79831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[INFO] df1.count() = 9999\n"
     ]
    }
   ],
   "source": [
    "###### Reading CSV data and write into DataFrame #######\n",
    "\n",
    "# Sample Customer Info Data\n",
    "\"\"\"\n",
    "cd /home/hduser/custinfo.csv\n",
    "\n",
    "4000001,Kristina,Chung,55,Pilot\n",
    "4000002,Paige,Chen,77,Teacher\n",
    "4000003,Sherri,Melton,34,Firefighter\n",
    "4000004,Gretchen,Hill,66,Computer hardware engineer\n",
    "4000005,Karen,Puckett,74,Lawyer\n",
    "\"\"\"\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Schema Definition\n",
    "custinfo_schema = StructType([StructField('custid', IntegerType(), True), StructField('first_name', StringType(), True), StructField('last_name', StringType(), True), StructField('age', IntegerType(), True), StructField('profession', StringType(), True)])\n",
    "\n",
    "# CSV Data Read and storing it in DataFrame\n",
    "df1 = spark.read.csv(path=\"file:///home/hduser/custinfo.csv\",header=False,sep=\",\",inferSchema=False,schema=custinfo_schema)\n",
    "df1.show(truncate=False,n=5)\n",
    "print(f\"[INFO] df1.count() = {df1.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d0aba4a-95f7-45ca-bfea-6324679cd6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CSV file data write into MySQL DB is successful.\n"
     ]
    }
   ],
   "source": [
    "###### Write the data into MySql DB ######\n",
    "\n",
    "# JDBC Options\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb?createDatabaseIfNotExist=true'\n",
    "dbproperties={'user':'root','password':'Root123$','driver':'com.mysql.cj.jdbc.Driver'}\n",
    "\n",
    "# Write into DB\n",
    "df1.write.jdbc(url=url1,properties=dbproperties,table=\"custinfo\",mode=\"overwrite\")\n",
    "print(\"[INFO] CSV file data write into MySQL DB is successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac17921-76c7-44e6-8fe1-138577b7f6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Simple way to read the data from MySql/RDBMS DB using JDBC ######\n",
    "\n",
    "# JDBC Options\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb'\n",
    "dbproperties={'user':'root','password':'Root123$','driver':'com.mysql.cj.jdbc.Driver'}\n",
    "\n",
    "# Read the data from RDBMS using query instead of direct table\n",
    "table_query = \"(select * from stocksdb.custinfo) as tablename\"\n",
    "df2_db = spark.read.jdbc(url=url1,properties=dbproperties,table=table_query)\n",
    "df2_db.cache()\n",
    "df2_db.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "210d08d4-984d-4630-bbb3-6b70ac7637f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Optimized way to read the data from any RDBMS DB using JDBC ######\n",
    "\n",
    "#Question: How to improve performance for JDBC?\n",
    "#partition, fetchsize, caching, pushdown optimization etc.,\n",
    "#partitionColumn:, numberOfPartitions:, upperBound:, lowerBound, predicates, fetchsize..\n",
    "\n",
    "# JDBC Options for performance optimization\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb'\n",
    "dbproperties = {\n",
    "    'user': 'root',\n",
    "    'password': 'Root123$',\n",
    "    'driver': 'com.mysql.cj.jdbc.Driver',\n",
    "    # Performance optimization options (values as strings):\n",
    "    'partitionColumn': 'custid',\n",
    "    'lowerBound': '4000001',  # Column used to divide data into sections for parallel processing.\n",
    "    'upperBound': '4000100',  # Minimum value for the partition column to start reading data.\n",
    "    'numPartitions': '3',     # Maximum value for the partition column to start reading data.\n",
    "    'pushDownPredicate': 'true',  # Sends filters (WHERE clauses) to the database for early processing.\n",
    "    'pushDownAggregate': 'true',  # Sends aggregations (SUM, COUNT) to the database for early processing.\n",
    "    'queryTimeout': '120',    # Maximum time (in seconds) a database query can run before timing out.\n",
    "    'fetchSize': '10',        # Number of rows retrieved from the database in each batch.\n",
    "    'isolationLevel': 'READ_COMMITTED' # Ensures only committed data is visible during a transaction.\n",
    "}\n",
    "\n",
    "# Read the data from RDBMS using query instead of direct table\n",
    "table_query = \"(select * from stocksdb.custinfo) as tablename\"\n",
    "df2_db = spark.read.jdbc(url=url1,properties=dbproperties,table=table_query)\n",
    "df2_db.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe16c11-95c0-4d85-8a2d-0daa71ac11e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
