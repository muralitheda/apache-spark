{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d55ae10-939c-4905-8f70-7f2a73350867",
   "metadata": {},
   "source": [
    "# **PySpark Ingestion + Egress + Dataloading Techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fa8ec8-e8f6-4c26-a269-20bad66f4c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/31 21:56:44 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.189.129 instead (on interface ens160)\n",
      "25/07/31 21:56:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/07/31 21:56:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] SparkSession Object Memory Reference: <pyspark.sql.session.SparkSession object at 0xffff80fbd3a0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#MySql jdbc connector jar local path\n",
    "mysql_connector_jar_path = \"/home/hduser/install/mysql-connector-java.jar\"\n",
    "\n",
    "#Spark Session Creation\n",
    "spark =  SparkSession.builder\\\n",
    "    .appName(\"Spark-Ingress-Egress-Dataloading-Practice\")\\\n",
    "    .config(\"spark.jars\", mysql_connector_jar_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"[INFO] SparkSession Object Memory Reference: {spark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f8fba-f5eb-422b-b9da-9a3aec6e3737",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **1. Reading a CSV data and write into MySql(RDBMS) Database using JDBC Option**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7fe856-1806-44e6-95d4-30b398a79831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[INFO] df1.count() = 9999\n"
     ]
    }
   ],
   "source": [
    "###### Reading CSV data and write into DataFrame #######\n",
    "\n",
    "# Sample Customer Info Data\n",
    "\"\"\"\n",
    "cd /home/hduser/custinfo.csv\n",
    "\n",
    "4000001,Kristina,Chung,55,Pilot\n",
    "4000002,Paige,Chen,77,Teacher\n",
    "4000003,Sherri,Melton,34,Firefighter\n",
    "4000004,Gretchen,Hill,66,Computer hardware engineer\n",
    "4000005,Karen,Puckett,74,Lawyer\n",
    "\"\"\"\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Schema Definition\n",
    "custinfo_schema = StructType([StructField('custid', IntegerType(), True), StructField('first_name', StringType(), True), StructField('last_name', StringType(), True), StructField('age', IntegerType(), True), StructField('profession', StringType(), True)])\n",
    "\n",
    "# CSV Data Read and storing it in DataFrame\n",
    "df1 = spark.read.csv(path=\"file:///home/hduser/custinfo.csv\",header=False,sep=\",\",inferSchema=False,schema=custinfo_schema)\n",
    "df1.show(truncate=False,n=5)\n",
    "print(f\"[INFO] df1.count() = {df1.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0aba4a-95f7-45ca-bfea-6324679cd6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] CSV file data write into MySQL DB is successful.\n"
     ]
    }
   ],
   "source": [
    "###### Write the data into MySql DB ######\n",
    "\n",
    "# JDBC Options\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb?createDatabaseIfNotExist=true'\n",
    "dbproperties={'user':'root','password':'Root123$','driver':'com.mysql.cj.jdbc.Driver'}\n",
    "\n",
    "# Write into DB\n",
    "df1.write.jdbc(url=url1,properties=dbproperties,table=\"custinfo\",mode=\"overwrite\")\n",
    "print(\"[INFO] CSV file data write into MySQL DB is successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac17921-76c7-44e6-8fe1-138577b7f6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Simple way to read the data from MySql/RDBMS DB using JDBC ######\n",
    "\n",
    "# JDBC Options\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb'\n",
    "dbproperties={'user':'root','password':'Root123$','driver':'com.mysql.cj.jdbc.Driver'}\n",
    "\n",
    "# Read the data from RDBMS using query instead of direct table\n",
    "table_query = \"(select * from stocksdb.custinfo) as tablename\"\n",
    "df2_db = spark.read.jdbc(url=url1,properties=dbproperties,table=table_query)\n",
    "df2_db.cache()\n",
    "df2_db.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "210d08d4-984d-4630-bbb3-6b70ac7637f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+---+--------------------------+\n",
      "|custid |first_name|last_name|age|profession                |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "|4000001|Kristina  |Chung    |55 |Pilot                     |\n",
      "|4000002|Paige     |Chen     |77 |Teacher                   |\n",
      "|4000003|Sherri    |Melton   |34 |Firefighter               |\n",
      "|4000004|Gretchen  |Hill     |66 |Computer hardware engineer|\n",
      "|4000005|Karen     |Puckett  |74 |Lawyer                    |\n",
      "+-------+----------+---------+---+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Optimized way to read the data from any RDBMS DB using JDBC ######\n",
    "\n",
    "#Question: How to improve performance for JDBC?\n",
    "#partition, fetchsize, caching, pushdown optimization etc.,\n",
    "#partitionColumn:, numberOfPartitions:, upperBound:, lowerBound, predicates, fetchsize..\n",
    "\n",
    "# JDBC Options for performance optimization\n",
    "url1='jdbc:mysql://127.0.0.1:3306/stocksdb'\n",
    "dbproperties = {\n",
    "    'user': 'root',\n",
    "    'password': 'Root123$',\n",
    "    'driver': 'com.mysql.cj.jdbc.Driver',\n",
    "    # Performance optimization options (values as strings):\n",
    "    'partitionColumn': 'custid',\n",
    "    'lowerBound': '4000001',  # Column used to divide data into sections for parallel processing.\n",
    "    'upperBound': '4000100',  # Minimum value for the partition column to start reading data.\n",
    "    'numPartitions': '3',     # Maximum value for the partition column to start reading data.\n",
    "    'pushDownPredicate': 'true',  # Sends filters (WHERE clauses) to the database for early processing.\n",
    "    'pushDownAggregate': 'true',  # Sends aggregations (SUM, COUNT) to the database for early processing.\n",
    "    'queryTimeout': '120',    # Maximum time (in seconds) a database query can run before timing out.\n",
    "    'fetchSize': '10',        # Number of rows retrieved from the database in each batch.\n",
    "    'isolationLevel': 'READ_COMMITTED' # Ensures only committed data is visible during a transaction.\n",
    "}\n",
    "\n",
    "# Read the data from RDBMS using query instead of direct table\n",
    "table_query = \"(select * from stocksdb.custinfo) as tablename\"\n",
    "df2_db = spark.read.jdbc(url=url1,properties=dbproperties,table=table_query)\n",
    "df2_db.show(truncate=False,n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c3ddf-7f63-496e-be76-c7515bee8d91",
   "metadata": {},
   "source": [
    "## **2. Schema Evoluation/Growing handling using columner file formats ORC/Parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8063682e-e89b-4789-9603-f9b093f76c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ORC/PARQUET Other Properties\n",
    "\n",
    "#Source is sending data on a daily basis, once in a while the schema of the data is evolving/growing\n",
    "  #Example (Day1): exch~stock~price\n",
    "  #Example (Day2): exch~stock~price~buyer\n",
    "  #Example (Day3): stock~price~seller\n",
    "\n",
    "#**mergeSchema: Orc/Parquet read all the datafiles headers and merge them into one header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d797ccc0-0dda-499b-9e55-a70c698013df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|exch|stock|price|\n",
      "+----+-----+-----+\n",
      "|NYSE|  CLI| 36.3|\n",
      "|NYSE|  ABC| 36.3|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+-----+-----+------+\n",
      "|exch|stock|price| buyer|\n",
      "+----+-----+-----+------+\n",
      "|NYSE|  CLI| 37.3|  Alan|\n",
      "|NYSE|  ABC| 37.3|Harpar|\n",
      "+----+-----+-----+------+\n",
      "\n",
      "+-----+-----+------+\n",
      "|stock|price|seller|\n",
      "+-----+-----+------+\n",
      "|  CLI| 37.3|  Jack|\n",
      "|  ABC| 37.3|  Ross|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "day1 = \"\"\"\n",
    "exch~stock~price\n",
    "NYSE~CLI~36.3\n",
    "NYSE~ABC~36.3\n",
    "\"\"\"\n",
    "\n",
    "day2 = \"\"\"\n",
    "exch~stock~price~buyer\n",
    "NYSE~CLI~37.3~Alan\n",
    "NYSE~ABC~37.3~Harpar\n",
    "\"\"\"\n",
    "\n",
    "day3 = \"\"\"\n",
    "stock~price~seller\n",
    "CLI~37.3~Jack\n",
    "ABC~37.3~Ross\n",
    "\"\"\"\n",
    "\n",
    "# Write the same data into ORC file format\n",
    "\n",
    "# Day 1\n",
    "lines_day1 = day1.strip().split('\\n')\n",
    "header_day1 = lines_day1[0].split('~')\n",
    "data_rows_day1 = [line.split('~') for line in lines_day1[1:]]\n",
    "df1 = spark.createDataFrame(data_rows_day1, header_day1)\n",
    "df1.show()\n",
    "df1.write.csv(path=\"file:///home/hduser/day1_csv\",mode=\"overwrite\",sep=\"~\")\n",
    "\n",
    "# Day 2\n",
    "lines_day2 = day2.strip().split('\\n')\n",
    "header_day2 = lines_day2[0].split('~')\n",
    "data_rows_day2 = [line.split('~') for line in lines_day2[1:]]\n",
    "df2 = spark.createDataFrame(data_rows_day2, header_day2)\n",
    "df2.show()\n",
    "df2.write.csv(path=\"file:///home/hduser/day2_csv\",mode=\"overwrite\",sep=\"~\")\n",
    "\n",
    "# Day 3\n",
    "lines_day3 = day3.strip().split('\\n')\n",
    "header_day3 = lines_day3[0].split('~')\n",
    "data_rows_day3 = [line.split('~') for line in lines_day3[1:]]\n",
    "df3 = spark.createDataFrame(data_rows_day3, header_day3)\n",
    "df3.show()\n",
    "df3.write.csv(path=\"file:///home/hduser/day3_csv\",mode=\"overwrite\",sep=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded00920-4510-4c1a-9995-fcfb62b9e00d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
